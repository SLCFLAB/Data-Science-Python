{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-hYvmAnT0J6"
      },
      "outputs": [],
      "source": [
        "# 차트에 한글이 나오게끔 한글 폰트 설치\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUziLQk-tiwn"
      },
      "source": [
        "폰트 설치가 끝났으면 런타임을 재시작해서 폰트가 적용되게 하고 다음 셀을 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l18LPLlFzaOU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.style.use('dark_background')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ref: 본 ipynb 파일은 다음 웹사이트를 참고했습니다.\n",
        "---\n",
        "\n",
        "2021.11.09 조준우 (metamath@gmail.com), https://metamath1.github.io \n",
        "\n",
        "https://metamath1.github.io/2021/11/11/transformer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqrfMxGZ72HW"
      },
      "source": [
        "## 트랜스포머를 공부하는 이유\n",
        "\n",
        "트랜스포머는 최근 가장 성공적인 모델임에도 불구하고 CNN, RNN에 대한 사전 지식이 전혀 필요없이 이해할 수 있다는 점에서 입문자분들이 꼭 공부하면 좋을 모델이라고 할 수 있습니다. 트랜스포머를 이해하기 위한 사전 지식은 다음 정도로 정리할 수 있습니다. \n",
        "\n",
        "- 신경망에서 완전 연결층 \n",
        "- 소프트맥스 함수\n",
        "- $\\sin$, $\\cos$ 함수 기초\n",
        "- 데이터를 인코딩해서 다시 디코딩하는 개념\n",
        "- 배치 정규화와 레이어 정규화의 차이\n",
        "- 임베딩 개념\n",
        "- 어텐션 개념\n",
        "\n",
        "앞 네 개 항목은 인공지능에 막 입문한 학습자라도 어느정도 알고 있는 내용이므로 실제로 입문자가 느끼는 추가 학습 부담은 정규화와 임베딩 개념 그리고 가장 핵심인 어텐션이라고 할 수 있습니다. 임베딩에 대해서는 입력을  특정 길이를 가지는 벡터로 바꾼다는 정도만 알아도 전체 내용을 이해하는데 지장이 없습니다. 어텐션에 대해서는 이전 모델인 seq2seq와 그에 대한 어텐션 매커니즘을 알고 있다면 도움이 되지만 몰라도 상관없습니다. \n",
        "\n",
        "이렇게 트랜스포머는 완전 연결층에 대한 기본적인 지식만 가지고 바로 학습할 수 있다는 장점에 더불어 최신 모델들의 근간을 이루는 기본 모델을 공부할 수 있다는 추가 장점도 가지고 있습니다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5_lwTI8gE0G"
      },
      "source": [
        "## 트랜스포머 개요\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1DudF_35VyVpevpsLr7TfVWi6WeC527kc)\n",
        "Figure 1: The Transformer - model architecture.[[4](https://arxiv.org/abs/1706.03762)]\n",
        "\n",
        "트랜스포머에 대한 그림 중 가장 유명한 그림이 바로 위 그림입니다. 이 그림은 처음 보면 아주 복잡하게 보이지만 트랜스포머를 이해하고 보면 간결하게 모델의 핵심을 잘 설명하고 있다는 것을 알게 됩니다. 이 그림을 통해서 먼저 트랜스포머에 대한 전체 맥락을 알아보겠습니다. 트랜스포머가 적용되는 상황은 기계번역, 즉 입력으로 영어 문장이 들어가서 출력으로 한국어 문장이 나오는 상황을 가정하겠습니다. 그러면 위 그림은 다음과 같은 순서로 데이터가 처리되는 것을 이야기하고 있습니다.\n",
        "\n",
        "1. 입력(Input)이 임베딩 층으로 입력되어 문장의 각 단어들이 적당한 벡터로 변환된다.\n",
        "2. 이 변환된 벡터가 위치 인코딩Positional Encoding이라는 것으로 부터 출력된 단어의 위치 정보를 가지는 벡터와 더해진다.\n",
        "3. 2에서 만들어진 벡터가 멀티 헤드 어텐션Multi-Head Attention이라는 층으로 입력되는데 이 때 같은 입력을 키key, 쿼리query, 벨류value로 넣어준다. \n",
        "4. 멀티 헤드 어텐션 층을 거쳐 출력된 벡터가 멀티 헤드 어텐션 층으로 입력되기 전 벡터와 더해지고(ResNet의 스킵커넥션) 레이어 정규화 된다.\n",
        "5. 4의 출력을 Linear 레이어에 입력하여 출력을 만든다. \n",
        "6. 5의 출력과 4의 출력을 더하고 레이어 정규화 한다. (또 한번 스킵커넥션)\n",
        "7. 6에서 출력된 결과를 입력으로 3으로 돌아가 다시 반복한다. 이렇게 N번 반복한다.\n",
        "\n",
        "여기까지가 트랜스포머의 인코더에 해당되는 내용입니다. 다음은 디코더에 대한 내용입니다.\n",
        "\n",
        "1. 입력과 쌍이 되는 정답을 디코더에 입력한다.\n",
        "2. 인코더와 마찬가지로 정답에 대해서 인코더의 1, 2 과정을 거친다.\n",
        "3. 이렇게 만들어진 벡터를 인코더의 3번, 4번 과정과 동일하게 어텐션한다. 단, 이때 마스크를 씌우게 되는데 이 마스크를 씌우는 부분이 트랜스포머를 이해하는 난관 중 하나입니다. 구체적인 세부 사항은 코드와 함께 설명하겠습니다. 여기서는 일단 어텐션 한다고 생각하면 되겠습니다.\n",
        "4. 3의 출력으로 다시 한번 어텐션하는데 이때는 키, 벨류는 인코더의 출력으로 설정하고 쿼리를 앞 3번 과정의 출력으로 설정하여 어텐션 한다. \n",
        "5. 4의 출력을 입력으로 Linear 레이어를 통과하고 스킵커넥션, 레이어 정규화를 적용한다.\n",
        "6. 5의 출력을 입력으로 3으로 돌아가 다시 반복한다. 이렇게 N번 반복한다.\n",
        "\n",
        "마지막으로 디코더의 내용으로 부터 클래스의 확률을 계산하는 부분은\n",
        "\n",
        "1. 디코더에서 6의 출력을 Linear층으로 입력한다.\n",
        "2. 1의 출력을 Softmax층으로 입력하여 클래스에 대한 확률을 계산한다.\n",
        "\n",
        "이고, 이것이 트랜스포머의 가장 핵심적인 내용입니다. 알고 보면 별것 아닌 내용인데 처음 공부하려면 꽤 복잡해 보이기도 한 것이 사실입니다. \n",
        "\n",
        "여기서 주의해야할 점은 트랜스포머에 입력은 타임스탭별로 하니씩 입력되지 않고 모든 타임스탭의 입력이 동시에 입력되고 출력도 마찬가지라는 점입니다. 예를 들어 다음과 같은 입력이 있을 때\n",
        "\n",
        "`<start>` `I` `don't` `know` `what` `you` `mean` `<end>`\n",
        "\n",
        "입력의 일곱개 단어가 모두 한번에 입력되고 여기에 대한 정답이 다음과 같을 때 \n",
        "\n",
        "`<start>` `나는` `니가` `의미하는` `바를` `모르겠다` `<end>`\n",
        "\n",
        "디코더의 입력으로는\n",
        "\n",
        "`<start>` `나는` `니가` `의미하는` `바를` `모르겠다`\n",
        "\n",
        "이 들어가고 출력으로는 \n",
        "\n",
        "`나는` `니가` `의미하는` `바를` `모르겠다` `<end>`\n",
        "\n",
        "가 나오기를 기대하는 것입니다. 물론 RNN을 사용한 seq2seq같은 모델을 아예 모르는 초심자라면 이런 내용에 주의를 할 필요 조차 없습니다. 그냥 입력이 문장 통째로 들어가서 출력이 문장 통째로 나온다고 생각하면 됩니다.\n",
        "\n",
        "이제 코드를 보면서 그림의 순서대로 따라가봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgX3hWSQRNH3"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWFwxsoy6WJv"
      },
      "source": [
        "트랜스포머에 입력 데이터인 단어를 입력하기 위해서는 단어에 대해 두가지 전처리를 거쳐야 하는데 하나는 임베딩이고 다른 하나는 위치 인코딩입니다. 모델에 단어를 입력할 때 단어를 숫자로 바꿔서 입력하게 되는데 이 때 가급적 목적하는 작업에 도움이 되도록 바꿔야 할 것입니다. 단어-숫자 변환을 최종 작업에 도움이 되는 방향으로 하기 위해 임베딩embedding이라는 층을 사용하게 됩니다. 임베딩 층은 단어를 입력받아 적절한 숫자 벡터로 변환하는 층으로 변환되어 출력되는 벡터의 길이를 코드에서 `d_model`로 표시합니다.\n",
        "\n",
        "임베딩층이 하는 일은 원핫인코딩된 입력벡터를 적절한 밀집벡터로 바꿔주는 것으로 다음처럼 작동합니다.<sup>&#8224;</sup> \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{\\text{emb}}=\\mathbf{x}W_{\\text{emb}}\n",
        "$$\n",
        "\n",
        "여기서 임베딩 층이 가지는 가중치 행렬 $W_{\\text{emb}}$의 크기는 $(\\text{vocab}, d_{\\text{model}})$이 되고 입력 벡터 $\\mathbf{x}$는 길이가 $(1, \\text{vocab})$인 원핫인코딩된 벡터입니다. 따라서 임베딩 층의 가중치 행렬은 특정 단어에 해당하는 $d_{\\text{model}}$차원의 벡터를 행으로 가지고 있는 룩업 테이블이라고 할 수 있습니다. 다음 그림처럼 단어장에서 단어의 순번을 가지고 변환될 벡터를 찾는 것입니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ajCVDKg_c8XQyZFUPr3kuAu3bFRnjAcG)\n",
        "\n",
        "그림을 보면 제일 먼저 'prime'이란 단어를 단어장에서 몇번째 단어인지 찾습니다. 그렇게 찾은 단어의 순번 자리만 1이 되고 나머지는 다 0인 전체 단어 개수만큼 요소를 가지는 원핫벡터를 만듭니다. 이 원핫벡터를 $W_{\\text{emb}}$에 행렬곱해서 임베딩 벡터를 얻는 것입니다.<sup>&#8224;&#8224;</sup>  이렇게 임베딩 층을 거치면 입력이 길이 $d_{\\text{model}}$인 벡터로 변환됩니다.\n",
        "\n",
        "---\n",
        "<sup>&#8224;</sup>  [1]에서는 가중치 행렬 $W$의 행수를 입력, 열수를 출력으로 적고 입력 $\\mathbf{x}$를 행벡터로 하여 $W$ 앞에서 곱하는 방식을 쓰고 있습니다. 다른 표현을 예로 들어보면 파이토치에서는 행벡터와 가중치 행렬의 곱을 $\\mathbf{x} W^{T}$로 표현을 합니다. 여기서 $W$는 행수를 출력, 열수를 입력으로 하고 있는 행렬이 됩니다. 개인적으로 가장 선호하는 방식은 가중치 행렬을 파이토치 형태로 쓰고 열 벡터 $\\mathbf{x}$를 뒤에서 곱하는 $W \\mathbf{x}$ 방식입니다.\n",
        "\n",
        "<sup>&#8224;&#8224;</sup>실제로는 이렇게 행렬곱하지 않습니다. 그냥 $W_{\\text{emb}}$에서 'prime'이라는 단어 인덱스에 해당하는 행을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ckIXaJBN0AD0"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 임베딩 벡터에 sqrt(d_model)을 곱해서 임베딩 벡터의 요소들 값을 \n",
        "        # 증가 시킴.\n",
        "        # d_model은 512정도 되는 큰 값이이므로 22정도 되는 값이 \n",
        "        # 임베딩 벡터 요소에 곱해짐. \n",
        "        # 곱하는 이유는 뒤에 포지션 벡터를 더할텐데 이 때 포지션 벡터에\n",
        "        # 의해 임베딩 결과가 희석되는 것을 막기 위함\n",
        "        # ref.: https://stackoverflow.com/questions/56930821/why-does-embedding-vector-multiplied-by-a-constant-in-transformer-model\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxe2mijnWm9e"
      },
      "source": [
        "코드는 파이토치 `nn.Embedding`을 그대로 사용합니다. 단 위 코드에서 임베딩 층을 포워드 시킨 결과에 $\\sqrt{d_{\\text{model}}}$을 곱하고 있는 점이 좀 특이합니다. 이렇게 상수를 곱하는 이유는 뒤에 설명할 포지션 벡터가 임베딩 벡터에 더해지는데 이 때 포지션 벡터가 더해지면서 임베딩 벡터의 값이 희석되는 것을 막기 위해 임베딩 벡터의 요소 크기를 상대적으로 크게 하기 위함일 것으로 생각됩니다.[[5](https://stackoverflow.com/questions/56930821/why-does-embedding-vector-multiplied-by-a-constant-in-transformer-model)] 이런 해석은 논문에서 공식적으로 밝히고 있는 것은 아닙니다. 논문에서는 그냥 $\\sqrt{d_{\\text{model}}}$를 곱한다고만 되어 있습니다.\n",
        "\n",
        "이 과정을 거쳐서 길이 $n_{\\text{seq}}$인 입력 문장의 각 단어를 임베딩 벡터로 모두 변환하게 되면 $(n_{\\text{seq}}, d_{\\text{model}})$인 행렬로 변환되게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAjuxxn8RUIW"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fdpGrwUsUCt"
      },
      "source": [
        "앞서 이야기한 위치 인코딩입니다. 트랜스포머는 RNN같은 순차적 구조를 사용하지 않고 문장 전체를 한꺼번에 입력하여 문장에 있는 단어간 관계를 특징화하는 모델입니다. RNN은 토큰이 순차적으로 입력되므로 토큰 순서 정보가 자연스럽게 생겨나는 반면 트랜스포머는 토큰의 순서 정보를 인위적으로 만들어 넣어줄 필요가 있습니다. 순서 정보를 인위적으로 만들기위해 논문 저자들은 위치 인코딩이라는 방법을 사용합니다. \n",
        "\n",
        "$$\n",
        "PE_{(pos,2i)}=\\sin \\left( \\frac{pos}{10000^{2i/d_{\\text{model}}}} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE_{(pos,2i+1)}=\\cos \\left( \\frac{pos}{10000^{2i/d_{\\text{model}}}} \\right)\n",
        "$$\n",
        "\n",
        "위 식을 사용하여 입력 문장의 위치 정보를 $(n_{\\text{seq}}, d_{\\text{model}})$인 행렬로 인코딩 합니다. 식을 보면 행은 단어의 위치를 나타내는데 단어 위치별 값은 $pos$ 변수에 의해 달라집니다. 열에 대해서는 홀수 열은 $\\cos$함수로 짝수 열은 $\\sin$함수를 사용해서 값을 다르게 계산하는데 열 인덱스 $i$에 대해서 각각 다른 주기(첫 두 열부터 $2 \\pi$ 주기에서 시작해서 마지막 두 열에서는 거의 $10000 \\times 2 \\pi$ 주기를 가짐) 의 삼각함수를 사용하여 모든 열에 대해서 다른 값을 계산하게 됩니다.\n",
        "\n",
        "문장 길이 120에 `d_model=512`인 경우 위치 인코딩을 직접 해보면 다음과 같습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "xBewD45hwAnk",
        "outputId": "b3927af8-a76e-4c53-e410-6199896b5589"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "seq_length = 120\n",
        "d_model = 512\n",
        "\n",
        "PE = np.zeros((seq_length, d_model))\n",
        "\n",
        "for pos in range(seq_length):\n",
        "    for i in range(d_model//2):\n",
        "        PE[pos, 2*i]   = np.sin( pos / (10000**(2*i/d_model)) )\n",
        "        PE[pos, 2*i+1] = np.cos( pos / (10000**(2*i/d_model)) )\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.imshow(PE, aspect='auto', cmap='gray', interpolation='gaussian')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8EW7-BqBg8y"
      },
      "source": [
        "그림으로 표시된 행렬의 각 행은 단어의 위치별로 다르게 계산된 숫자 512개로 구성되어 있고 이 행 벡터가 단어의 위치 정보를 나타내는 고유한 값이 되는 것입니다.\n",
        "\n",
        "논문에서는 위치 인코딩을 어떤 방법으로 해도 상관없다고 이야기합니다. 토큰 위치에 따라 유일하게 다른 값들을 계산해주는 함수라면 뭐든 가능하단 이야기입니다. sin, cos같은 주기 함수를 사용하면 위치에 따라 서로 다른 값을 계산할 수 있는 것 이외에도 상대위치의 선형성을 확보할 수 있다는 장점도 있습니다. 상대위치의 선형성을 자세히 설명한 글은 이 블로그[[6](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)]를 참고하시면 좋겠습니다.\n",
        "\n",
        "여기서는 간단한 실험으로 상대위치의 선형성을 직관적으로 알아보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yJFeOstIzeeZ"
      },
      "outputs": [],
      "source": [
        "def pe_test(pos, i, d_model=512):\n",
        "    PE = []\n",
        "    PE.append( np.sin( pos / (10000**(2*i/d_model)) ) )\n",
        "    PE.append( np.cos( pos / (10000**(2*i/d_model)) ) )\n",
        "\n",
        "    return np.array(PE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPUOViPw0B6y",
        "outputId": "ccbc89a1-cee6-4f0c-c88a-2cc4dbd8662f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe at 100: [ 0.79754236 -0.60326294]\n",
            "pe at 110: [-0.64526647  0.76395758]\n",
            "linear transformation of pe at 100 by M: [-0.64526647  0.76395758]\n",
            "pe at 172: [ 0.55020692 -0.83502835]\n",
            "pe at 182: [-0.3529983   0.93562396]\n",
            "linear transformation of pe at 172 by M: [-0.3529983   0.93562396]\n"
          ]
        }
      ],
      "source": [
        "k = 10\n",
        "d_model = 512\n",
        "i = 1\n",
        "\n",
        "pos1 = 100\n",
        "pos1k = pos1 + k\n",
        "\n",
        "pos2 = 172\n",
        "pos2k = pos2 + k\n",
        "\n",
        "M_k = np.array([[np.cos(k/(10000**(2*i/d_model))), np.sin(k/(10000**(2*i/d_model)))], \n",
        "                [-np.sin(k/(10000**(2*i/d_model))), np.cos(k/(10000**(2*i/d_model)))]])\n",
        "\n",
        "pe_pos1 = pe_test(pos1, i)\n",
        "pe_pos1k = pe_test(pos1k, i)\n",
        "pe_pos2 = pe_test(pos2, i)\n",
        "pe_pos2k = pe_test(pos2k, i)\n",
        "\n",
        "print(f'pe at {pos1}:', pe_pos1)\n",
        "print(f'pe at {pos1k}:', pe_pos1k)\n",
        "print(f'linear transformation of pe at {pos1} by M:', np.dot(M_k, pe_pos1))\n",
        "\n",
        "print(f'pe at {pos2}:', pe_pos2)\n",
        "print(f'pe at {pos2k}:', pe_pos2k)\n",
        "print(f'linear transformation of pe at {pos2} by M:', np.dot(M_k, pe_pos2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD2i1Sw69nNl"
      },
      "source": [
        "위 소스를 보면 위치 100에서 위치인코딩을 한 값과 `k=10`만큼 떨어진 위치 110에서 위치인코딩 값을 출력합니다. 그리고 위치 100에서 위치 인코딩 값에서 변환형렬 `M_k`을 곱한 값도 함께 출력합니다. 값을 비교해보면 100에서 10만큼 떨어진 위치 110에서 인코딩 값이 100에서 위치 인코딩 값을 M을 사용해 선형변환한 값과 똑같은 것을 알 수 있습니다. 동일한 실험을 기준 위치 172에서 해도 172에서 위치 인코딩 값에 `M_k`을 곱한 갑과 172+10에서 위치 인코딩 값이 동일한 것을 확인할 수 있습니다. 즉 어떤 위치를 기준으로 잡든 `k`(offset)만큼 떨어진 위치에서 인코딩 값은\n",
        "\n",
        "$$\n",
        "PE_{\\text{pos}+k} = M(k) \\times PE_{\\text{pos}}\n",
        "$$\n",
        "\n",
        "라는 선형관계에 있게 되는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppsnR1894K0Z"
      },
      "source": [
        "위 코드셀에서는 논문에 나온 수식을 그대로 구현하였고 아래 코드에서는 수치 계산에서 안정성을 위해 논문에 식을 그대로 사용하지 않고 아래처럼 변환해서 구현합니다.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{pos}{10000^{2i/d_{\\text{model}}}} \n",
        "&= pos \\times \\left[ \\exp\\left\\{{\\log\\left(  \\frac{1}{10000^{2i/d_{\\text{model}}}}\\right)} \\right\\} \\right] \\\\\n",
        "&= pos \\times \\left[ \\exp \\left\\{ \\log \\left( \\left( 10000^{2i/d_{\\text{model}}} \\right)^{-1}  \\right) \\right\\} \\right]\\\\\n",
        "&= pos \\times \\left[ \\exp\\left\\{\\log \\left( 10000^{-2i/d_{\\text{model}}} \\right)\\right\\} \\right] \\\\\n",
        "&= pos \\times \\left[ \\exp\\left\\{ -\\frac{2i}{d_{\\text{model}}} \\log(10000) \\right\\} \\right]\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X-jRt5-U0BXe"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        pe_val = self.pe[:, :x.size(1)]\n",
        "        pe_val.requires_grad = False\n",
        "        \n",
        "        x = x + pe_val\n",
        "        # x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "\n",
        "        # page 7\n",
        "        # In addition, we apply dropout to the sums of the embeddings and the\n",
        "        # positional encodings in both the encoder and decoder stacks.\n",
        "        # For the base model, we use a rate of Pdrop=0.1.\n",
        "        return self.dropout(x)\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELQTovh2tT9t"
      },
      "source": [
        "그리고 마지막 출력을 `nn.Dropout`을 통과시켜 규제regularization 효과를 주고 있습니다.\n",
        "\n",
        "지금까지 설명한 임베딩과 위치 인코딩이 어떻게 작동하는지 실제 간단한 예로 알아보겠습니다. 다음 코드는 열한개 단어를 사용하여 문장을 만들 때 각 단어를 길이 10인 벡터로 임베딩하는 임베딩 층과 이에 대한 위치 인코딩층을 실험한 코드입니다. 코드에 자세한 주석을 달았고 주석을 보면 알 수 있지만 실제 단어로 구성된 문장이 아닌 숫자가 단어라고 가정한 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "oenqh38-ESNf",
        "outputId": "e50f7623-7385-4548-c5bc-7cd29632b851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "integer tokens: tensor([[8, 4, 6, 8, 1]])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x216 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.random.manual_seed(15)\n",
        "\n",
        "# 단어장 크기 11, 임베딩 벡터 길이 10인 임베딩 층 선언\n",
        "V = 11\n",
        "d_model = 10\n",
        "emb = Embeddings(d_model, V)\n",
        "# 길이 10인 문장의 위치를 인코딩하는 위치 인코딩 층 선언\n",
        "pe  = PositionalEncoding(d_model, 0.1)\n",
        "# 그림 그리기 위해 positional encoding을 복사해둠\n",
        "pe_ = pe.pe.clone()\n",
        "\n",
        "# 문장 길이는 5이고\n",
        "n_seq = 5\n",
        "# 0~10까지 숫자 5개를 무작위로 뽑아서 문장을 구성\n",
        "# 이 예에서 문장은 실제 단어로 구성된 것은 아니고 0, 1, 2, 3, ..., 10인 \n",
        "# 숫자를 단어로 간주함\n",
        "x = torch.randint(0, V-1, (1, n_seq,), requires_grad=False)\n",
        "print(\"integer tokens:\", x)\n",
        "\n",
        "# Feed forward Embeddings-PositionalEncoding\n",
        "# 숫자(단어) 다섯개로 구성된 입력을 임베딩층을 통해 (n_seq, d_model)로 변환\n",
        "embedded = emb(x)\n",
        "\n",
        "# 임베딩 벡터에 대한 위치 인코딩 정보를 구해서 임베딩 벡터에 더함\n",
        "embedded_pe = pe(embedded)\n",
        "\n",
        "# 임베딩 벡터와 위치 인코딩이 더해진 입력 벡터를 그림 \n",
        "fig, axs = plt.subplots(figsize=(18,3), nrows=1, ncols=3)\n",
        "axs[0].imshow(embedded.detach().numpy()[0], aspect='auto', cmap='gray')\n",
        "axs[0].set_xlabel('d_model')\n",
        "axs[0].set_ylabel('n_seq')\n",
        "axs[0].set_title(f\"embedded vector x\")\n",
        "\n",
        "axs[1].imshow(pe_.numpy()[0][:x.shape[1]], aspect='auto', cmap='gray')\n",
        "axs[1].set_xlabel('d_model')\n",
        "axs[1].set_ylabel('n_seq')\n",
        "axs[1].set_title(f\"positional encoding\")\n",
        "\n",
        "axs[2].imshow(embedded_pe.detach().numpy()[0], aspect='auto', cmap='gray')\n",
        "axs[2].set_xlabel('d_model')\n",
        "axs[2].set_ylabel('n_seq')\n",
        "axs[2].set_title(f\"embedded vector x + positional encoding\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suX7jeqbv4Z2"
      },
      "source": [
        "오른쪽 그림을 보면 위치 정보가 임베딩 벡터에 대해져서 원래 임베딩 벡터와는 색이 조금씩 달라진 것을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7folB27QqXR6"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "이제 인코더로 데이터를 입력하기 위한 준비를 마쳤으니 인코더 구조를 알아봅시다. 글 시작 부분에 제시된 트랜스포머의 그림에서 Multi-Head Attention, Add & Norm 부분을 조금 더 자세히 그리면 아래와 같습니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=10OW0QpQ5jTiW_K__FjmNZphB3fKuSacy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCSpqUBUTOWO"
      },
      "source": [
        "임베딩과 포지션 인코딩이 끝난 입력이 Q, K, V라는 세 입력으로 인코더로 들어갑니다. 일반적으로 Q(쿼리), K(키), V(벨류)는 다른 값이 되어야 하지만 여기선 셀프 어텐션을 수행하기 때문에 같은 값으로 입력됩니다. 어텐션은 트랜스포머 이전까지 인코더와 디코더 간에 일어나는 연산으로 개발되었으며 셀프 어텐션에 대응되는 용어로 크로스 어텐션이라고 부르기도 합니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZLYYG2ktDCk"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOedrrabjOr1"
      },
      "source": [
        "\n",
        "어텐션의 기본적인 동작 방식은 쿼리를 가지고 키로 질의를 해서 얻은 어텐션 가중치를 벨류에 적용하는 것입니다. 보통 어텐션에서 키란 인코더에서 생성한 정보로 이 정보를 디코더에게 효율적으로 전달하는 것이 어텐션의 주 목적입니다. 왜냐하면 디코더는 좋은 결과를 생성하기 위해 인코더의 정보를 가능한 많이 활용하는 편이 도움이 되기 때문입니다. 이전 seq2seq는 인코더가 인코딩한 마지막 결과만 디코더로 전달하기 때문에 디코더 입장에서는 좋은 결과를 생성하기 힘든 것이죠. 전체적인 과정은 대략 다음처럼 진행됩니다.\n",
        "\n",
        "1. 인코더는 인코딩 중에 생성된 정보 다시말해 키를 모두 가지고 있습니다.\n",
        "2. 디코더가 정보를 생성할 때 디코더가 만든 중간 결과 즉 쿼리를 가지고 이 쿼리와 가장 관계가 높은 정보가 어떤 것인지 인코더 쪽에 물어 봅니다. \n",
        "3. 인코더는 자기가 인코딩하면서 생성한 정보(키)들 중에 요청받은 쿼리와 어떤 키가 얼마나 적합한지 가중치 계산을 합니다.\n",
        "4. 인코더는 이렇게 계산된 가중치를 디코더로 전달할 벨류라는 값에 가중합하여 디코더로 전달합니다.\n",
        "5. 디코더는 4에서 전달받은 정보와 현재 디코더가 디코딩한 정보를 합하여 최종 결과를 만들어 냅니다.\n",
        "\n",
        "이렇게 크로스 어텐션인 경우 대충 아이디어만 들어도 충분히 성능 향상에 영향을 미치겠다는 생각이 듭니다. 하지만 트랜스포머는 이 어텐션 아이디어를 인코더 또는 디코더에만 적용하는 식으로 적용범위를 넓혔습니다. 인코더는 입력 정보끼리 어텐션을 계산해서 디코더가 디코딩하기 좋은 인코딩 정보를 만들어 낼 수 있고 디코더도 마찬가지로 디코딩되는 정보 끼리 어텐션을 해서 더 좋은 결과를 만들어 낼수 있게 되는 것입니다. 이런 어텐션을 셀프 어텐션이라 합니다.\n",
        "\n",
        "셀프 어텐션은 논문에서 제시하는 다음 수식으로 수행됩니다.\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IRPp4bJbVGi"
      },
      "source": [
        "이렇게 셀프 어텐션이 되면 어떤 점이 좋아지는지 개념을 이해하기 위해 간단한 그림을 보도록 하겠습니다.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c_TAUg5FY3YNknOMKZMW8XGGbwTNGlzF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyKVcGSb8wI"
      },
      "source": [
        "위 그림은 i love you so much라는 토큰들이 임베딩되는 상황을 나타냅니다. 우선 $W^Q_i$, $W^K_i$, $W^V_i$를 각각 곱하여 $Q$, $K$, $V$를 만들고 그렇게 만들어진 텐서는 색깔을 다르게 표현했습니다. 이후 $\\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)$가 실행되면 $(n_{\\text{seq}}, n_{\\text{seq}})$인 `p_attn` 행렬이 만들어지는데 이 행렬은 각 토큰들간의 관계가 계산되도록  $W^Q_i$, $W^K_i$가 학습되길 바라는 것입니다.  $W^Q_i$, $W^K_i$가 잘 학습되어 i love you so much라는 문장에서 각 토큰간의 관계가 잘 만들어졌다고 가정하겠습니다. 그러면 love라는 토큰은 한 문장안에서 love, you 라는 토큰하고 관계가 높게 표현될 수 있습니다. 그림에서  `p_attn`을 보면 love에 해당하는 행에서 love와 you에 해당하는 열이 붉게 빛나는 것을 볼 수 있습니다. 이렇게 인코딩된 어텐션 행렬과 $V$를 곱하게 됩니다. 행렬곱은 앞에서 곱하는 행렬의 행으로 뒤에서 곱하는 행렬의 행을 선형조합하는 것과 같습니다. 최종적으로 계산된 `head_i`에서 love에 해당하는 행은 $V$ 행렬의 행 다섯 개가 선형조합된 것인데 이때 $V$에서 love와 you에 해당하는 행에 높은 가중치가 부여되어 조합된 벡터가 됩니다. \n",
        "\n",
        "이렇게 셀프 어텐션되어 출력되는 결과는 각 토큰이 `d_model`사이즈로 변환된 벡터를 가지는데 그치지 않고 각 벡터들이 서로 관계가 높은 토큰들끼리 잘 조합되어 만들어진 벡터가 되게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayUe45lhbTjk"
      },
      "source": [
        "아래 코드는 어텐션을 구현한 것입니다. 쿼리, 키, 벨류로 무엇을 입력는가에 따라 셀프 어텐션과 크로스 어텐션으로 나눌 수 있으므로 구현 코드는 두 경우 모두 동일합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0a6MWdYRz6Hb"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    # 아래서 nbatches는 미니배치 크기, 코드에서 nbatches라는 변수명을 쓰므로\n",
        "    # 표기를 통일하기 위해 nbatches로 표기\n",
        "    # query: (nbatches, h, n_seq, d_k)\n",
        "    # key:   (nbatches, h, n_seq, d_k)\n",
        "    # value: (nbatches, h, n_seq, d_v) 인데 d_k=d_v로 두었음\n",
        "    # 이 함수는 아래쪽 MultiHeadedAttention 클래스의 foward 함수에서 호출됨\n",
        "\n",
        "    d_k = query.size(-1)\n",
        "    \n",
        "    # Scaled에 대한 여러 참고 링크들\n",
        "    # https://stats.stackexchange.com/questions/318243/variance-and-expectation-of-dot-product\n",
        "    # https://www.reddit.com/r/learnmath/comments/9gbk4q/mean_and_variance_of_dot_product_of_two_random/\n",
        "    # https://stats.stackexchange.com/questions/52646/variance-of-product-of-multiple-random-variables\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    # scores: (nbatches, h, n_seq, n_seq)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    # torch.matmul(p_attn, value): (nbatches, h, n_seq, n_seq)*(nbatches, h, n_seq, d_v)\n",
        "    # = (nbatches, h, n_seq, d_v),      p_attn: (nbatches, h, n_seq, nseq)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daF4FY2Anpzi"
      },
      "source": [
        "위 과정을 Scaled-dot product attention이라고 하는데 $\\sqrt{d_k}$로 나누는 부분 때문에 scaled라는 이름이 붙었습니다. $\\sqrt{d_k}$로 나누는 이유는 softmax를 취한 후 값들이 극단적으로 0과 1이 몰리게 되는 것을 막기 위함입니다. 만약 이런 현상이 일어난다면 softmax를 미분했을 때 미분 계수가 거의 0이 되어 그래디언트를 구하기 힘들어지는 문제가 생깁니다. 좀 더 자세한 설명은 아래 기술하였습니다. \n",
        "\n",
        "바쁘신 분들은 스킵해도 좋겠습니다. 🤨\n",
        "\n",
        "인코더는 N개가 누적되면서 인코더의 출력이 다시 인코더의 입력으로 들어가는 구조로 되어 있습니다. 그림을 보면 인코더가 인코딩을 출력하기 전에 레이어 정규화층을 거치게 됩니다. 즉 셀프 어텐션을 위해 입력되는 $Q$, $K$는 레이어 정규화를 거치게 되므로 $Q$, $K$의 행벡터 $\\mathbf{q}$와 $\\mathbf{k}$의 요소들은 대략적으로 평균 0, 분산 1을 따른다고 가정할 수 있습니다.\n",
        "\n",
        "$$\n",
        "q_i , k_i \\sim p(0, 1)\n",
        "$$\n",
        "\n",
        "행렬곱 $Q K^T$에서 $Q$의 길이 $d_k$인 행벡터 $\\mathbf{q}$와 $K$의 열벡터 $\\mathbf{k}$가 내적되는데 이 때 벡터의 요소가 서로 독립이고 위처럼 평균 0, 분산 1을 따른다고 가정하면 $\\mathbf{q} \\cdot \\mathbf{k}$는 평균 0, 분산 $d_k$를 가지게 됩니다. 이는 다음처럼 보일 수 있습니다.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbb{E}\\left[\\mathbf{q} \\cdot \\mathbf{k}\\right] \n",
        "&= \\mathbb{E}\\left[ \\sum_{i=1}^{d_k} q_i k_i \\right] \\\\\n",
        "&= \\sum_{i=1}^{d_k} \\mathbb{E} \\left[ q_i k_i \\right]\n",
        "\\end{aligned} \\tag{1}\n",
        "$$\n",
        "\n",
        "위 식(1)에서 기댓값의 성질 $\\mathbb{E}[X+Y] = \\mathbb{E}[X]+\\mathbb{E}[Y]$가 사용되었습니다. 이제 시그마 안쪽 $\\mathbb{E} \\left[ q_i k_i \\right]$를 전개해보면\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbb{E}[q_i k_i] \n",
        "&= \\sum_{q_i} \\sum_{k_i} q_i  k_i \\ p(q_i, k_i) \\\\\n",
        "&= \\sum_{q_i} \\sum_{k_i} q_i  k_i \\ p(q_i) p(k_i)  \\quad \\because q_i \\text{ and } k_i \\text{ are independent}  \\\\\n",
        "&= \\sum_{q_i} q_i \\, p(q_i) \\sum_{k_i} k_i \\, p(k_i) \\\\\n",
        "&= \\mathbb{E}[q_i] \\, \\mathbb{E}[k_i] = 0 \\quad \\because \\mathbb{E}[q_i] = \\mathbb{E}[k_i] = 0\n",
        "\\end{aligned} \\tag{2}\n",
        "$$\n",
        "\n",
        "식(2)의 결과를 식(1)에 대입하면 \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbb{E}\\left[\\mathbf{q} \\cdot \\mathbf{k}\\right] \n",
        "&= \\mathbb{E}\\left[ \\sum_{i=1}^{d_k} q_i k_i \\right] \\\\\n",
        "&= \\sum_{i=1}^{d_k} \\mathbb{E} \\left[ q_i k_i \\right] \\\\\n",
        "&= \\sum_{i=1}^{d_k} 0 = 0\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "이 되게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUr0Ihbhj091"
      },
      "source": [
        "분산도 위와 비슷한 과정으로 $d_k$가 됨을 보일 수 있습니다.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{Var}\\left[\\mathbf{q} \\cdot \\mathbf{k}\\right] \n",
        "&= \\text{Var}\\left[\\sum_{i=1}^{d_k} q_i k_i \\right]  \\\\\n",
        "&= \\sum_{i=1}^{d_k} \\text{Var} \\left[ q_i k_i \\right]\n",
        "\\end{aligned} \\tag{3}\n",
        "$$\n",
        "\n",
        "식(3)에서 독립인 확률변수에 대한 분산의 성질 $\\text{Var}[X+Y] = \\text{Var}[X] + \\text{Var}[Y]$가 사용되었습니다. \n",
        "\n",
        "이제 기댓값처럼 시그마 안쪽을 전개하면\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{Var}\\left[ q_i k_i\\right] \n",
        "&= \\mathbb{E}[q_i^2 k_i^2]-\\left(\\mathbb{E}[q_i k_i ]\\right)^2 \\\\[5pt]\n",
        "&= \\mathbb{E}\\left[q_i^2\\right] \\mathbb{E}\\left[ k_i^2 \\right] - \\mathbb{E}\\left[q_i\\right]^2 \\mathbb{E} [k_i]^2 \\\\[5pt]\n",
        "&=\\left( \\text{Var}[q_i]+\\mathbb{E}[q_i]^2 \\right) \\left( \\text{Var}[k_i]+\\mathbb{E}[k_i]^2 \\right) - \\mathbb{E}\\left[q_i\\right]^2 \\mathbb{E} [k_i]^2 \\\\[5pt]\n",
        "&= \\text{Var}[q_i] \\text{Var}[k_i] + \\text{Var}[q_i] \\mathbb{E} [k_i]^2 + \\mathbb{E} [q_i]^2 \\text{Var}[k_i] + \\mathbb{E}\\left[q_i\\right]^2 \\mathbb{E} [k_i]^2 - \\mathbb{E}\\left[q_i\\right]^2 \\mathbb{E} [k_i]^2 \\\\[5pt]\n",
        "&= \\text{Var}[q_i] \\text{Var}[k_i] + \\text{Var}[q_i] \\mathbb{E} [k_i]^2 + \\mathbb{E} [q_i]^2 \\text{Var}[k_i]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "여기서 각 요소들은 $ \\mathbb{E} [q_i] = \\mathbb{E} [k_i] = 0 $ , $\\text{Var}[q_i] = \\text{Var}[k_i] = 1$이므로\n",
        "\n",
        "$$\n",
        "\\text{Var}\\left[ q_i k_i\\right] = \\text{Var}[q_i] \\text{Var}[k_i] = 1 \\tag{4}\n",
        "$$\n",
        "\n",
        "이 되고 식(4)를 윗 식에 대입하면\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{Var}\\left[\\mathbf{q} \\cdot \\mathbf{k}\\right] \n",
        "&= \\text{Var}\\left[\\sum_{i=1}^{d_k} q_i k_i \\right]  \\\\\n",
        "&= \\sum_{i=1}^{d_k} \\text{Var} \\left[ q_i k_i \\right] \\\\\n",
        "&= \\sum_{i=1}^{d_k}  \\text{Var}[q_i] \\text{Var}[k_i] \\\\\n",
        "&= \\sum_{i=1}^{d_k} 1 = d_k\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "이 됨을 알 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYtrEdRMs1AG"
      },
      "source": [
        "두 행렬을 곱한 행렬의 요소 $\\left(Q K^T \\right)_{ij}$는 $\\mathbf{q}_i \\cdot \\mathbf{k}_j$로 계산되는데 앞선 계산에서 본것 처럼 분산이 $d_k$가 되므로 $d_k$가 커질 수록 $\\left(Q K^T \\right)_{ij}$의 값들의 차이도 커지게 됩니다. $\\left(Q K^T \\right)_{ij}$들의 분산이 커지면 $ Q K^T  $가 소프트맥스 함수를 거친 후 요소의 값은 1에 아주 가깝거나 0에 아주 가까운 값들로 구성되게 될 것입니다. 한편 소프트맥스 $\\mathbf{s}(\\mathbf{z})$의 미분은 다음과 같습니다.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial z_j} s_i(\\mathbf{z})=\\begin{cases}\n",
        "s_j(\\mathbf{z}) (1-s_j(\\mathbf{z})), & \\mbox{if } i = j \\\\\n",
        "-s_i(\\mathbf{z})s_j(\\mathbf{z}), & \\mbox{if } i \\neq j\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "위 식을 보면 소프트맥스 함수의 미분계수는 결국 소프트맥스 함수의 요소들 끼리 곱으로 결정되는데 이 요소들이 0에 아주 가깝거나 1에 아주 가깝게 되면 미분계수가 매우 작아지는 문제가 생기게 됩니다. 결과적으로 소프트맥스 함수를 백워드 패스할 때 미분계수가 매우 작아져서 학습에 문제가 생기게 되겠죠. 그래서 앞서 구한 $\\left(Q K^T \\right)_{ij}$의 표준편차인 $\\sqrt{d_{\\text{k}}}$로 나눠서 평균과 분산을 0, 1에 가깝게 만들려하는 것입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zv0_nz0t3vk"
      },
      "source": [
        "#### Multi-Head Attention\n",
        "\n",
        "앞 절에서 이야기한 어텐션을 한번만 하는 것이 아니라 $h$번하고 그 결과로 나온 $(n_{seq}, d_{v})$인 결과 $h$개를 $d_v$ 방향으로 `concat`시켜 최종적으로 $(n_{seq}, hd_{v})$ 만들게 됩니다. 위 인코더 그림에서 이를 표현하고 있습니다. \n",
        "\n",
        "그리고 이 $(n_{seq}, hd_{v})$를 $(hd_{v}, d_{model})$인 $W^o$와 곱하여 결과를 $(n_{seq}, d_{model})$로 만들게 됩니다. 아래 그 코드가 있는데 매우 교묘하게 코딩되어 있어 주의깊게 볼 필요가 있습니다. 우선 코드에 적혀있는 주석을 읽고 코드를 이해해봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AQTK7gaSzmVa"
      },
      "outputs": [],
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zUsjvdDQz8S_"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h  # Q, K의 차원\n",
        "        self.h = h\n",
        "        \n",
        "        # Wq, Wk, Wv and Wo\n",
        "        # Wq, Wk, Wv를 각각 h개 만들지 않고 \n",
        "        # Wq, Wk, Wv를 d_model의 1/h 크기로 만듬\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
        "        \n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query, key, value: (n_seq, d_model)\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # EncoderLayer에서 호출될 때\n",
        "            # mask는 src_mask: (nbatches, 1, n_seq_src)\n",
        "            # DecoderLayer에서 호출될 때\n",
        "            # self_attn으로 호출되면 mask는 tgt_mask: (nbatches, n_seq_trg, n_seq_trg)\n",
        "            # src_attn(cross_attn)으로 호출되며 mask는 src_mask: (nbatches, 1, n_seq_src)\n",
        "            \n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        # self.linears는 요소 네갠데 (query, key, value)와 짝을 맞춰서\n",
        "        # 루프는 총 3번 돌아감\n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        # 이 라인이 실행되면 query, key, value는 각각\n",
        "        # (nbatches, h, n_seq, d_k) 가 됨\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        # x: (nbatches, h, n_seq, d_v),  self.attn: (nbatches, h, n_seq, n_seq)\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        # x: (nbatches, n_seq, h*d_k) 여기서 h*d_k=d_model\n",
        "\n",
        "        # 4) matmul x and Wo -> (nbatches, n_seq, d_model)\n",
        "        return self.linears[-1](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_f04zeIo4GD"
      },
      "source": [
        "위 코드에서 가장 난해한 코드는 주석 1) 부분입니다. \n",
        "\n",
        "```python\n",
        "# 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "# self.linears는 요소 네갠데 (query, key, value)와 짝을 맞춰서\n",
        "# 루프는 총 3번 돌아감\n",
        "query, key, value = \\\n",
        "    [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        for l, x in zip(self.linears, (query, key, value))]\n",
        "# 이 라인이 실행되면 query, key, value는 각각\n",
        "# (nbatches, h, n_seq, d_k) 가 됨\n",
        "```\n",
        "\n",
        "1)에서 $(n_{seq}, d_{k})$ 모양의 쿼리, 키, 벨류 $h$개를 동시에 만들고 있습니다. 쿼리를 만드는 과정을 아래 그림으로 자세히 나타냈습니다. 나머지 키와 벨류에 대해서도 동일한 연산이 수행됩니다.\n",
        "  \n",
        "![picture](https://drive.google.com/uc?id=10JMUOLLUgyPJwqQvVyWG2J9MmUIB_gEo)\n",
        "\n",
        "그림 상단에 코드가 주석 1)에 해당하는 코드이며 코드 부분이 진행되는 순서대로 텐서를 표현하였습니다. 1)에서 결과를 앞서 알아본 `attention` 함수로 입력하여 어텐션 결과와 어텐션 맵을 돌려 받는 부분이 2)입니다. 그런 다음 3)에서 $h$개 어텐션 결과를 `concat`하고 이를 4)에서 $W^O$와 곱하게 됩니다.\n",
        "\n",
        "마지막 완전연결층의 가중치 $W^{O}$는 $(hd_v , d_{\\text{model}})$ 크기를 가지므로 이 층의 입력차원은 $hd_v$, 출력차원은 $d_{\\text{model}}$이 됩니다. 입력으로 들어오는 멀티 헤드 어텐션의 출력은 $(n_{\\text{seq}} , hd_k)$입니다. 셀프 어텐션에서 $d_k=d_v$이므로 마지막 완전연결층을 통과하면 출력은 $(n_{\\text{seq}},d_{\\text{model}})$이 됩니다. \n",
        "\n",
        "위 멀티헤드어텐션 `forward()`에서 마지막 줄입니다.\n",
        "\n",
        "```python\n",
        "# matmul x and Wo -> (nbatches, n_seq, d_model)\n",
        "return self.linears[-1](x)\n",
        "```\n",
        "\n",
        "3)과 4) 과정을 아래 그림으로 나타내었습니다. 인코더의 전체 그림과 함께 비교하면 이해하기가 훨씬 쉬워집니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=10KXLvpL_UU1NbWQT5sTlaiF5EHtQdhek)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqd8GdVgtUTO"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd4oK0hZCjKv"
      },
      "source": [
        "앞선 단계를 거쳐 얻은 결과는 모양이 $(\\text{nbatches}, n_{\\text{seq}}, d_{\\text{model}})$이고 인코더의 입력도 모양이 $(\\text{nbatches}, n_{\\text{seq}}, d_{\\text{model}})$이므로 서로 요소끼리 더할 수 있습니다. 이렇게 더하는 과정을 스킵커넥션skip-connection이라고 합니다. 스킵 커넥션은 ResNet[[6](https://arxiv.org/abs/1512.03385)]에서 소개된 기법으로 이 후 모델에서는 거의 필수로 사용되는 기법입니다. 이렇게 스킵커넥션까지 거친 텐서를 정규화하게 되는데 여기서는 보편적으로 사용하는 배치정규화를 쓰지 않고 레이어 정규화를 사용합니다. \n",
        "\n",
        "배치 정규화와 레이어 정규화의 차이를 알아보기 위해 구체적인 예를 들어 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkZRxTeu7RwM"
      },
      "source": [
        "#### Batch Normalization\n",
        "\n",
        "배치 정규화에 대한 수식은 아래와 같습니다.\n",
        "\n",
        "$$\n",
        "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}} * \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "식만보면 입력 배치 $x$에 대한 평균과 분산을 구해서 입력들에 대해서 표준화 작업을 하는 것입니다. 그런데 PyTorch 구현체를 보면 상황이 조금 더 복잡합니다.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1BdMW-mYOwNea6yH6wyk63ZPCBSsHLGij)\n",
        "\n",
        "위 그림은 데이터 하나가 요소 4개짜리 벡터이고 미니배치 크기가 3인 상황을 나타내었습니다. 이 경우 입력 텐서는 (3,4) 또는 (3,1,4)입니다. 트랜스포머로 입력되는 상황을 고려하면 (3,1,4)가 적합한데 각 차원은 (nbatches, n_seq, embedding)으로 샘플은 세 개, 각 샘플당 토큰은 한 개, 각 토큰은 숫자 네 개로 임베딩되어 있는 상황입니다. 노란색, 주황색, 보라색 데이터가 각각 네트워크에 입력되고 길이 여섯개짜리 벡터로 변환되면 출력은 (3,1,6)이 됩니다.   \n",
        "\n",
        "이 출력에 배치 정규화를 적용하면 `(N, C, L)`에서 `C`에 대해 데이터를 구분해서 `C`별로 평균과 분산을 구하게 됩니다. 이것은 이미지에 대해서 배치 정규화를 적용하면 이미지의 채널별로 평균과 분산을 구하는 것과 동일한 방식입니다.\n",
        "\n",
        "실제로 그렇게 작동하는지 아래 pytorch 코드와 직접 만든 배치 정규화 코드의 결과를 비교하였습니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWbJcixn8iqt",
        "outputId": "15437014-8e76-403c-b9b2-b82265f26a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT torch.Size([3, 1, 4])\n",
            "tensor([[[ 1.5410, -0.2934, -2.1788,  0.5684]],\n",
            "\n",
            "        [[-1.0845, -1.3986,  0.4033,  0.8380]],\n",
            "\n",
            "        [[-0.7193, -0.4033, -0.5966,  0.1820]]])\n",
            "\n",
            "ACTIVATION torch.Size([3, 1, 6])\n",
            "tensor([[[-1.6115,  0.2043,  0.3828, -0.4272, -0.1355, -0.9343]],\n",
            "\n",
            "        [[-0.4144,  0.7838,  0.4598,  0.5056,  0.1713,  1.1928]],\n",
            "\n",
            "        [[-0.5397,  0.4543,  0.7689,  0.4446,  0.1475,  0.2965]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "PYTORCH BATCH NORM. 1D\n",
            "tensor([[[-2.5974,  0.1627,  0.4341, -0.7971, -0.3537, -1.5679]],\n",
            "\n",
            "        [[-0.7777,  1.0436,  0.5512,  0.6207,  0.1127,  1.6653]],\n",
            "\n",
            "        [[-0.9681,  0.5429,  1.0210,  0.5281,  0.0764,  0.3030]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "\n",
            "MY BATCH NORM.\n",
            "mean shape: torch.Size([1, 1])\n",
            "var shape: torch.Size([1, 1])\n",
            "tensor([[[-2.5974,  0.1627,  0.4341, -0.7971, -0.3537, -1.5679]],\n",
            "\n",
            "        [[-0.7777,  1.0436,  0.5512,  0.6207,  0.1127,  1.6653]],\n",
            "\n",
            "        [[-0.9681,  0.5429,  1.0210,  0.5281,  0.0764,  0.3030]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# 배치사이즈 3, 단어 한개로 구성된 문장, feature 4\n",
        "x = torch.randn(3, 1, 4)\n",
        "print('\\nINPUT', x.shape)\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION', x.shape)\n",
        "print(x)\n",
        "\n",
        "#===================\n",
        "# batch norm.\n",
        "# pytorch \n",
        "bnorm = nn.BatchNorm1d(x.shape[1], eps=0., momentum=0.)\n",
        "print('\\nPYTORCH BATCH NORM. 1D')\n",
        "print(bnorm(x))\n",
        "\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=(0,2)).reshape(-1,1)\n",
        "var = x.var(axis=(0,2), unbiased=False).reshape(-1,1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY BATCH NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)\n",
        "#===================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ekyQ_lieG8"
      },
      "source": [
        "두 경우 모두 출력이 동일하고 평균과 분산은 숫자 하나라는 사실을 알 수 있습니다. (이 예에서 C=1이기 때문입니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC8knQQGtzUQ"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1RBZJ6wWqq5PABCW8Yw5-Ny9yupcxNRqA)\n",
        "\n",
        "이번에는 데이터가 위처럼 입력된다고 생각해봅시다. 이 상황은 샘플 세 개가 입력되는데 전과는 다르게 각 데이터는 토큰이 두 개로 구성된 문장이고 토큰 하나는 숫자 네 개짜리 벡터로 표현된 상황입니다. 이번 예에서는 C=2이기 때문에 각 샘플의 첫번째 단어끼리 평균과 분산을 구하고, 두번째 단어끼리 평균과 분산을 구해서 각 단어에 대해 표준화를 수행할 것입니다.\n",
        "\n",
        "아래 코드를 보면 그 사실을 확인할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlNATxHvJcbc",
        "outputId": "efff8072-5ce2-429f-ebf7-a9ca16766e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT torch.Size([3, 2, 4])\n",
            "tensor([[[ 5.4329e-01, -3.9516e-01,  2.0553e-01, -4.5033e-01],\n",
            "         [-5.7308e-01, -5.5536e-01,  5.9432e-01,  1.5419e+00]],\n",
            "\n",
            "        [[-1.0925e+00, -8.5194e-02, -9.3348e-02,  6.8705e-01],\n",
            "         [-8.3832e-01,  8.9182e-04,  8.4189e-01, -4.0003e-01]],\n",
            "\n",
            "        [[ 6.2114e-01,  6.3818e-01, -2.4600e-01,  2.3025e+00],\n",
            "         [-1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01]]])\n",
            "\n",
            "ACTIVATION torch.Size([3, 2, 6])\n",
            "tensor([[[-0.0636, -0.0346,  0.0337, -0.0617, -0.7393,  0.5536],\n",
            "         [ 0.8464,  1.3011,  0.0941,  0.5593,  0.2273,  0.2352]],\n",
            "\n",
            "        [[ 0.6898,  0.7568,  0.4834,  0.5283,  0.2946, -0.0747],\n",
            "         [ 0.6558,  0.7065, -0.0185,  0.2920, -0.5097, -0.1648]],\n",
            "\n",
            "        [[-0.2619,  0.3526,  0.0770, -0.2704,  0.9377,  0.2629],\n",
            "         [ 0.6593,  0.0531,  1.1190,  0.7146,  0.0106, -0.2787]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "PYTORCH BATCH NORM. 1D\n",
            "tensor([[[-0.6198, -0.5497, -0.3845, -0.6153, -2.2559,  0.8745],\n",
            "         [ 1.0204,  1.9767, -0.5619,  0.4166, -0.2817, -0.2651]],\n",
            "\n",
            "        [[ 1.2042,  1.3664,  0.7044,  0.8132,  0.2472, -0.6468],\n",
            "         [ 0.6196,  0.7262, -0.7988, -0.1456, -1.8319, -1.1065]],\n",
            "\n",
            "        [[-1.1000,  0.3878, -0.2796, -1.1207,  1.8043,  0.1706],\n",
            "         [ 0.6268, -0.6482,  1.5938,  0.7432, -0.7376, -1.3461]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "\n",
            "MY BATCH NORM.\n",
            "mean shape: torch.Size([2, 1])\n",
            "var shape: torch.Size([2, 1])\n",
            "tensor([[[-0.6198, -0.5497, -0.3845, -0.6153, -2.2559,  0.8745],\n",
            "         [ 1.0204,  1.9767, -0.5619,  0.4166, -0.2817, -0.2651]],\n",
            "\n",
            "        [[ 1.2042,  1.3664,  0.7044,  0.8132,  0.2472, -0.6468],\n",
            "         [ 0.6196,  0.7262, -0.7988, -0.1456, -1.8319, -1.1065]],\n",
            "\n",
            "        [[-1.1000,  0.3878, -0.2796, -1.1207,  1.8043,  0.1706],\n",
            "         [ 0.6268, -0.6482,  1.5938,  0.7432, -0.7376, -1.3461]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 배치사이즈 3, 단어 두 개로 구성된 문장, feature 4\n",
        "x = torch.randn(3, 2, 4)\n",
        "print('\\nINPUT', x.shape)\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION', x.shape)\n",
        "print(x)\n",
        "\n",
        "#===================\n",
        "# batch norm.\n",
        "# pytorch \n",
        "bnorm = nn.BatchNorm1d(x.shape[1], eps=0., momentum=0.)\n",
        "print('\\nPYTORCH BATCH NORM. 1D')\n",
        "print(bnorm(x))\n",
        "\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=(0,2)).reshape(-1,1)\n",
        "var = x.var(axis=(0,2), unbiased=False).reshape(-1,1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY BATCH NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)\n",
        "#==================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmp1erKejHfB"
      },
      "source": [
        "직접 구현한 코드를 보면 평균과 분산이 숫자 두개로 구성된 것을 알 수 있습니다. 이 두 경우 모두 구해진 평균과 분산에서 미니배치의 차원인 3이 사라진 것을 확인할 수 있습니다. 미니배치 내 샘플들끼리 평균과 분산을 구했기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0Bjbgh9IkDe"
      },
      "source": [
        "만약 첫번째 예와 같은 상황에서 입력을 (3,4)로 했다면 평균과 분산을 구하는 방식이 조금 달라지게 됩니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1hRLd1iV5wlLDF__VD4HtozaKnNgdgQYv)\n",
        "\n",
        "길이 6짜리 벡터를 더해서 평균벡터와 분산벡터를 구하는 상황입니다. 이런 식으로 동작하기 위해서 `(N,L)`에서 `L`을 `BatchNorm1d()`에 넘기면 됩니다. 아래 실험 코드가 있습니다. 이런 경우를 pytorch 문서에서는 'Temporal Batch Normalization'이라고 부릅니다. 미니배치에 있는 샘플들을 대상으로 `C`차원에 대해서 각각 구분해서 평균과 분산을 구해야 하는데 샘플에 `C`차원이 없어서 이렇게 이름 붙인것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3ikbGuO_IcG",
        "outputId": "5384f098-b8f1-4122-b6eb-14e14d0eb440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT torch.Size([3, 4])\n",
            "tensor([[ 1.8929,  3.1110, -0.4584, -0.3360],\n",
            "        [-1.5700,  1.2315,  1.3946,  1.1711],\n",
            "        [ 0.4335, -1.7343, -1.3360,  0.8871]])\n",
            "\n",
            "ACTIVATION torch.Size([3, 6])\n",
            "tensor([[-0.8547,  0.8803, -0.9615,  0.1794, -0.1448, -1.3131],\n",
            "        [-0.7770,  0.4903, -1.8076, -0.7847, -0.0231,  0.8162],\n",
            "        [ 1.2591, -1.1274,  0.3239,  0.6613,  0.3677, -1.1282]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "PYTORCH BATCH NORM. 1D\n",
            "tensor([[-0.7464,  0.9195, -0.1671,  0.2673, -0.9670, -0.8009],\n",
            "        [-0.6670,  0.4708, -1.1326, -1.3363, -0.4102,  1.4099],\n",
            "        [ 1.4135, -1.3903,  1.2997,  1.0690,  1.3772, -0.6089]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "\n",
            "MY BATCH NORM.\n",
            "mean shape: torch.Size([6])\n",
            "var shape: torch.Size([6])\n",
            "tensor([[-0.7464,  0.9195, -0.1671,  0.2673, -0.9670, -0.8009],\n",
            "        [-0.6670,  0.4708, -1.1326, -1.3363, -0.4102,  1.4099],\n",
            "        [ 1.4135, -1.3903,  1.2997,  1.0690,  1.3772, -0.6089]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 배치사이즈 3, 단어 한개로 구성된 문장, feature 4, 여기선 C가 없음\n",
        "x = torch.randn(3, 4)\n",
        "print('\\nINPUT', x.shape)\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION', x.shape)\n",
        "print(x)\n",
        "\n",
        "\n",
        "# pytorch \n",
        "bnorm = nn.BatchNorm1d(x.shape[1], eps=0., momentum=0.)\n",
        "print('\\nPYTORCH BATCH NORM. 1D')\n",
        "print(bnorm(x))\n",
        "\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=0)\n",
        "var = x.var(axis=0, unbiased=False)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY BATCH NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqfuardnJroG"
      },
      "source": [
        "출력 샘플 하나가 길이 6짜리 벡터이므로 이런식으로 동작하는것이 좀 더 일반적인 경우라 할 수 있습니다. 버클리 대학교 인공지능 수업 [[cs182](https://cs182sp21.github.io/)]에서도 배치 정규화를 정확히 이렇게 설명하고 있습니다. 아래는 설명을 발췌한 슬라이드인데 수식을 보면 그냥 미니배치 안의 샘플들끼리 평균을 구하고 표준편차를 구하는 것을 확인할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1YYnMN9hD6xJY5zNFvEG1nBslg4caMsVm)\n",
        "\n",
        "\n",
        "이제 배치놈에 대해서 자세히 알아봤으니 레이어 정규화에 대해서 알아볼 차례입니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UaGFngvorJh"
      },
      "source": [
        "#### Layer Normalization\n",
        "\n",
        "레이어 정규화는 배치 정규화보다 개념이 간단합니다. 그냥 출력층 모양에 맞춰서 평균과 분산을 구하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBNh-Ryctc4N"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1jBGGDQPHCWppGLMuY0Nw0J48AYQ6SEHe)\n",
        "\n",
        "위 그림이라면 출력층이 숫자 여섯개인 벡터이므로 출력도 여섯개씩 묶어서 평균과 분산을 구하면 됩니다. 아래 코드에 실험결과가 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSYqF1MflWc6",
        "outputId": "4a8fec38-d508-47fc-fa4a-eeafda450a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT torch.Size([3, 1, 4])\n",
            "tensor([[[ 1.5410, -0.2934, -2.1788,  0.5684]],\n",
            "\n",
            "        [[-1.0845, -1.3986,  0.4033,  0.8380]],\n",
            "\n",
            "        [[-0.7193, -0.4033, -0.5966,  0.1820]]])\n",
            "\n",
            "ACTIVATION torch.Size([3, 1, 6])\n",
            "tensor([[[-1.6115,  0.2043,  0.3828, -0.4272, -0.1355, -0.9343]],\n",
            "\n",
            "        [[-0.4144,  0.7838,  0.4598,  0.5056,  0.1713,  1.1928]],\n",
            "\n",
            "        [[-0.5397,  0.4543,  0.7689,  0.4446,  0.1475,  0.2965]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "PYTORCH LAYER NORM.\n",
            "tensor([[[-1.7441,  0.9143,  1.1757, -0.0102,  0.4169, -0.7525]],\n",
            "\n",
            "        [[-1.7336,  0.6699,  0.0201,  0.1118, -0.5586,  1.4903]],\n",
            "\n",
            "        [[-1.9794,  0.4748,  1.2514,  0.4509, -0.2829,  0.0851]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "MY LAYER NORM.\n",
            "mean shape: torch.Size([3, 1, 1])\n",
            "var shape: torch.Size([3, 1, 1])\n",
            "tensor([[[-1.7441,  0.9143,  1.1757, -0.0102,  0.4169, -0.7525]],\n",
            "\n",
            "        [[-1.7336,  0.6699,  0.0201,  0.1118, -0.5586,  1.4903]],\n",
            "\n",
            "        [[-1.9794,  0.4748,  1.2514,  0.4509, -0.2829,  0.0851]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# 배치사이즈 3, 단어 한개로 구성된 문장, feature 4\n",
        "x = torch.randn(3, 1, 4)\n",
        "print('\\nINPUT', x.shape)\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION', x.shape)\n",
        "print(x)\n",
        "\n",
        "#===================\n",
        "# layer norm.\n",
        "# pytorch \n",
        "# 정규화할 shape을 출력레이어 모양에 맞게 설정한다.\n",
        "lnorm = nn.LayerNorm(x.shape[2], eps=0.)\n",
        "print('\\nPYTORCH LAYER NORM.')\n",
        "print(lnorm(x))\n",
        "\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=-1).reshape(-1,1,1)\n",
        "var = x.var(axis=-1, unbiased=False).reshape(-1,1,1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY LAYER NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)\n",
        "#===================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkBUeGbRpLCx"
      },
      "source": [
        "그림처럼 출력을 층 모양에 맞게 묶어서 평균과 분산을 구하면 숫자가 세 개 구해지게 됩니다. 위 코드의 출력에서 마지막에 구한 `mean`, `var`의 `shape`을 확인해보면 숫자 세 개로 구성된 것을 알 수 있습니다. 적절한 자리에 빼고, 나누고 하기 위해 `reshape`을 적당히 해주면 pytorch 결과와 똑같은 결과를 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElOAxxKZSgpm"
      },
      "source": [
        "레이어 정규화를 두번째 예에 적용해보면 다음처럼 각 토큰별로 계산하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl3ZJIt1tcxx"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1TXKgQMek9cq4a0-fS_krZ1U3lTIZO7mO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tZgt1i4nBGF",
        "outputId": "07144e31-7b6a-40c6-9a4f-f5dc38395b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT torch.Size([3, 2, 4])\n",
            "tensor([[[ 5.4329e-01, -3.9516e-01,  2.0553e-01, -4.5033e-01],\n",
            "         [-5.7308e-01, -5.5536e-01,  5.9432e-01,  1.5419e+00]],\n",
            "\n",
            "        [[-1.0925e+00, -8.5194e-02, -9.3348e-02,  6.8705e-01],\n",
            "         [-8.3832e-01,  8.9182e-04,  8.4189e-01, -4.0003e-01]],\n",
            "\n",
            "        [[ 6.2114e-01,  6.3818e-01, -2.4600e-01,  2.3025e+00],\n",
            "         [-1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01]]])\n",
            "\n",
            "ACTIVATION torch.Size([3, 2, 6])\n",
            "tensor([[[-0.0636, -0.0346,  0.0337, -0.0617, -0.7393,  0.5536],\n",
            "         [ 0.8464,  1.3011,  0.0941,  0.5593,  0.2273,  0.2352]],\n",
            "\n",
            "        [[ 0.6898,  0.7568,  0.4834,  0.5283,  0.2946, -0.0747],\n",
            "         [ 0.6558,  0.7065, -0.0185,  0.2920, -0.5097, -0.1648]],\n",
            "\n",
            "        [[-0.2619,  0.3526,  0.0770, -0.2704,  0.9377,  0.2629],\n",
            "         [ 0.6593,  0.0531,  1.1190,  0.7146,  0.0106, -0.2787]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "pytorch layer norm.\n",
            "tensor([[[-0.0308,  0.0462,  0.2279, -0.0258, -1.8293,  1.6118],\n",
            "         [ 0.7188,  1.7990, -1.0687,  0.0366, -0.7522, -0.7334]],\n",
            "\n",
            "        [[ 0.8809,  1.1233,  0.1340,  0.2965, -0.5493, -1.8854],\n",
            "         [ 1.1331,  1.2490, -0.4087,  0.3013, -1.5316, -0.7432]],\n",
            "\n",
            "        [[-1.0792,  0.4115, -0.2572, -1.1000,  1.8309,  0.1939],\n",
            "         [ 0.5762, -0.6729,  1.5234,  0.6902, -0.7604, -1.3565]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "my layer norm.\n",
            "mean shape: torch.Size([3, 2, 1])\n",
            "var shape: torch.Size([3, 2, 1])\n",
            "tensor([[[-0.0308,  0.0462,  0.2279, -0.0258, -1.8293,  1.6118],\n",
            "         [ 0.7188,  1.7990, -1.0687,  0.0366, -0.7522, -0.7334]],\n",
            "\n",
            "        [[ 0.8809,  1.1233,  0.1340,  0.2965, -0.5493, -1.8854],\n",
            "         [ 1.1331,  1.2490, -0.4087,  0.3013, -1.5316, -0.7432]],\n",
            "\n",
            "        [[-1.0792,  0.4115, -0.2572, -1.1000,  1.8309,  0.1939],\n",
            "         [ 0.5762, -0.6729,  1.5234,  0.6902, -0.7604, -1.3565]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 배치사이즈 3, 단어 두 개로 구성된 문장, feature 4\n",
        "x = torch.randn(3, 2, 4)\n",
        "print('\\nINPUT', x.shape)\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION', x.shape)\n",
        "print(x)\n",
        "\n",
        "# pytorch \n",
        "lnorm = nn.LayerNorm(x.shape[2], eps=0.)\n",
        "print('\\npytorch layer norm.')\n",
        "print(lnorm(x))\n",
        "\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=-1).unsqueeze(-1)\n",
        "var = x.var(axis=-1, unbiased=False).unsqueeze(-1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nmy layer norm.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5L35Mg9r98c"
      },
      "source": [
        "NLP에서는 시퀀스의 길이가 샘플마다 모두 달라서 패딩을 하게 되고 또 시퀀스 길이가 대체로 길어서 미니배치 사이즈를 크게 가져가지 못하기 때문에 배치 정규화를 사용하기 부적합하다고 판단하고 레이어 정규화를 사용했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f6IND4rKzp_O"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "\n",
        "        # torch.nn.LayerNorm()과 맞추기 위해 unbiased=False로 수정\n",
        "        std = x.std(-1, unbiased=False, keepdim=True)\n",
        "        # std = x.std(-1, keepdim=True)\n",
        "        \n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4uqOeU7spJ8"
      },
      "source": [
        "위에서 만든 레이어 정규화 층을 테스트 해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWZBASxl6VTt",
        "outputId": "10db5be0-561c-4a62-8484-e7364e04dd47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT\n",
            "tensor([[[ 1.5410, -0.2934, -2.1788,  0.5684]],\n",
            "\n",
            "        [[-1.0845, -1.3986,  0.4033,  0.8380]],\n",
            "\n",
            "        [[-0.7193, -0.4033, -0.5966,  0.1820]]])\n",
            "\n",
            "ACTIVATION\n",
            "torch.Size([3, 1, 6])\n",
            "\n",
            "PYTORCH LAYER NORM.\n",
            "tensor([[[-1.7441,  0.9143,  1.1757, -0.0102,  0.4169, -0.7525]],\n",
            "\n",
            "        [[-1.7336,  0.6699,  0.0201,  0.1118, -0.5586,  1.4903]],\n",
            "\n",
            "        [[-1.9794,  0.4748,  1.2514,  0.4509, -0.2829,  0.0851]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "MY LAYER NORM.\n",
            "mean shape: torch.Size([3, 1, 1])\n",
            "var shape: torch.Size([3, 1, 1])\n",
            "tensor([[[-1.7441,  0.9143,  1.1757, -0.0102,  0.4169, -0.7525]],\n",
            "\n",
            "        [[-1.7336,  0.6699,  0.0201,  0.1118, -0.5586,  1.4903]],\n",
            "\n",
            "        [[-1.9794,  0.4748,  1.2514,  0.4509, -0.2829,  0.0851]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "class LayerNorm\n",
            "tensor([[[-1.7441,  0.9143,  1.1757, -0.0102,  0.4169, -0.7525]],\n",
            "\n",
            "        [[-1.7336,  0.6699,  0.0201,  0.1118, -0.5586,  1.4903]],\n",
            "\n",
            "        [[-1.9794,  0.4748,  1.2514,  0.4509, -0.2829,  0.0851]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# 배치사이즈 3, 단어 한개로 구성된 문장, feature 4\n",
        "x = torch.randn(3, 1, 4)\n",
        "print('\\nINPUT')\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION')\n",
        "print(x.shape)\n",
        "\n",
        "#===================\n",
        "# layer norm.\n",
        "# pytorch \n",
        "lnorm2 = nn.LayerNorm(x.shape[-1], eps=0.)\n",
        "print('\\nPYTORCH LAYER NORM.')\n",
        "print(lnorm2(x))\n",
        "\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=-1).reshape(-1,1,1)\n",
        "var = x.var(axis=-1, unbiased=False).reshape(-1,1,1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY LAYER NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)\n",
        "\n",
        "# 레이어 층\n",
        "lnorm = LayerNorm(x.shape[-1], eps=0.)\n",
        "print('\\nclass LayerNorm')\n",
        "print(lnorm(x))\n",
        "\n",
        "\n",
        "#===================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DfhA-Hatc6E"
      },
      "source": [
        "pytorch의 `nn.LayerNorm()`과 앞서 직접 만들어본 레이어 정규화 결과와 클래스로 제공된 코드의 결과가 동일한 것을 알 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGrIJKhoVth5"
      },
      "source": [
        "#### Instance Normalization\n",
        "\n",
        "참고 삼아 인스턴스 정규화에 대한 실험 코드도 아래 제시 했습니다. 인스턴스 정규화는 배치 차원과 채널 차원을 제외한 차원에 대해서 평균을 내는 방식입니다. 지금까지 사용한 예를 들어 알아보면 데이터가 (3,2,4)일 때 인스턴스 정규화를 적용하면 (3,2,$\\cdot$)차원에 대해서는 계산하지 않고 마지막 ($\\cdot$, $\\cdot$,4)차원인 네 개 숫자를 모두 더해서 4로 나눈 것이 평균이 됩니다. 그래서 평균과 분산은 (3,2,1)이 되고 이를 각 샘플과 채널에 대해서 정규화 하는 방식입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNEkIwdtGddZ",
        "outputId": "66bc910b-f689-4e51-a546-c2976b28c2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "INPUT torch.Size([3, 2, 4])\n",
            "tensor([[[ 0.0930, -0.6661,  0.6080, -0.7300],\n",
            "         [-0.8834, -0.4189, -0.8048,  0.5656]],\n",
            "\n",
            "        [[ 0.2886,  0.3866, -0.2011, -0.1179],\n",
            "         [-0.8294, -1.4073,  1.6268,  0.1723]],\n",
            "\n",
            "        [[-0.7043,  0.3147,  0.1574,  0.3854],\n",
            "         [ 0.5737,  0.9979,  0.5436,  0.0788]]])\n",
            "\n",
            "ACTIVATION torch.Size([3, 2, 6])\n",
            "tensor([[[-0.6992,  0.6678, -0.1870,  1.0235, -0.1628, -0.0693],\n",
            "         [-0.0563,  0.2134,  0.4644, -0.0637, -0.5647, -0.2266]],\n",
            "\n",
            "        [[-0.4087,  0.0637,  0.6507,  0.2064, -0.4659,  0.1965],\n",
            "         [-0.6933,  1.3719, -0.9119,  1.2849,  0.3551, -0.6392]],\n",
            "\n",
            "        [[-0.3181,  0.5679,  0.1869,  0.2197, -0.3134, -0.1508],\n",
            "         [-0.5591,  0.2020,  0.6163,  0.2718, -0.2213,  0.2627]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "PYTORCH BATCH NORM. 1D\n",
            "tensor([[[-1.7095,  1.3849, -0.5500,  2.1901, -0.4952, -0.2837],\n",
            "         [-0.1905,  0.2458,  0.6519, -0.2026, -1.0131, -0.4661]],\n",
            "\n",
            "        [[-1.0518,  0.0173,  1.3462,  0.3405, -1.1814,  0.3180],\n",
            "         [-1.2213,  2.1202, -1.5749,  1.9796,  0.4750, -1.1336]],\n",
            "\n",
            "        [[-0.8467,  1.1587,  0.2963,  0.3706, -0.8363, -0.4681],\n",
            "         [-1.0041,  0.2275,  0.8978,  0.3403, -0.4575,  0.3256]]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n",
            "\n",
            "MY BATCH NORM.\n",
            "mean shape: torch.Size([2, 1])\n",
            "var shape: torch.Size([2, 1])\n",
            "tensor([[[-1.7095,  1.3849, -0.5500,  2.1901, -0.4952, -0.2837],\n",
            "         [-0.1905,  0.2458,  0.6519, -0.2026, -1.0131, -0.4661]],\n",
            "\n",
            "        [[-1.0518,  0.0173,  1.3462,  0.3405, -1.1814,  0.3180],\n",
            "         [-1.2213,  2.1202, -1.5749,  1.9796,  0.4750, -1.1336]],\n",
            "\n",
            "        [[-0.8467,  1.1587,  0.2963,  0.3706, -0.8363, -0.4681],\n",
            "         [-1.0041,  0.2275,  0.8978,  0.3403, -0.4575,  0.3256]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "PYTORCH INSTANCE NORM. 1D\n",
            "tensor([[[-1.3786,  0.9928, -0.4900,  1.6098, -0.4480, -0.2859],\n",
            "         [-0.0536,  0.7789,  1.5539, -0.0766, -1.6231, -0.5794]],\n",
            "\n",
            "        [[-1.1707,  0.0605,  1.5908,  0.4327, -1.3200,  0.4067],\n",
            "         [-0.8753,  1.3258, -1.1082,  1.2332,  0.2421, -0.8176]],\n",
            "\n",
            "        [[-1.0876,  1.6647,  0.4811,  0.5831, -1.0733, -0.5679],\n",
            "         [-1.7183,  0.2800,  1.3676,  0.4630, -0.8314,  0.4392]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "MY INSTANCE NORM.\n",
            "mean shape: torch.Size([3, 2, 1])\n",
            "var shape: torch.Size([3, 2, 1])\n",
            "tensor([[[-1.3786,  0.9928, -0.4900,  1.6098, -0.4480, -0.2859],\n",
            "         [-0.0536,  0.7789,  1.5539, -0.0766, -1.6231, -0.5794]],\n",
            "\n",
            "        [[-1.1707,  0.0605,  1.5908,  0.4327, -1.3200,  0.4067],\n",
            "         [-0.8753,  1.3258, -1.1082,  1.2332,  0.2421, -0.8176]],\n",
            "\n",
            "        [[-1.0876,  1.6647,  0.4811,  0.5831, -1.0733, -0.5679],\n",
            "         [-1.7183,  0.2800,  1.3676,  0.4630, -0.8314,  0.4392]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# 배치사이즈 3, 단어 두 개로 구성된 문장, feature 4\n",
        "x = torch.randn(3, 2, 4)\n",
        "print('\\nINPUT', x.shape)\n",
        "print(x)\n",
        "\n",
        "# 4차원 벡터를 6차원 벡터로 변환하는 완전연결 층\n",
        "linear = nn.Linear(x.shape[-1], 6)\n",
        "x = linear(x)\n",
        "print('\\nACTIVATION', x.shape)\n",
        "print(x)\n",
        "\n",
        "#===================\n",
        "# batch norm.\n",
        "# pytorch \n",
        "bnorm = nn.BatchNorm1d(x.shape[1], eps=0., momentum=0.)\n",
        "print('\\nPYTORCH BATCH NORM. 1D')\n",
        "print(bnorm(x))\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=(0,2)).reshape(-1,1)\n",
        "var = x.var(axis=(0,2), unbiased=False).reshape(-1,1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY BATCH NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)\n",
        "#===================\n",
        "\n",
        "#===================\n",
        "# instance norm.\n",
        "# pytorch \n",
        "inorm = nn.InstanceNorm1d(x.shape[1], eps=0., momentum=0.)\n",
        "print('\\nPYTORCH INSTANCE NORM. 1D')\n",
        "print(inorm(x))\n",
        "# 직접 만들기\n",
        "mean = x.mean(axis=2).unsqueeze(2)#.reshape(-1,1)\n",
        "var = x.var(axis=2, unbiased=False).unsqueeze(2)#.reshape(-1,1)\n",
        "gamma = torch.ones(x.shape)\n",
        "beta = torch.zeros(x.shape)\n",
        "print('\\nMY INSTANCE NORM.')\n",
        "print('mean shape:', mean.shape)\n",
        "print('var shape:', var.shape)\n",
        "print(((x - mean) / torch.sqrt(var))*gamma + beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YpOFnAEVsDz"
      },
      "source": [
        "\n",
        "이렇게 인코더를 구상하기 위한 기본 요소를 모두 알아봤습니다. 이제 이것들을 모아서 인코더를 만들어볼 차례입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3nqJcxtkeN"
      },
      "source": [
        "### EncoderLayer와 Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgBBKFT1W3DC"
      },
      "source": [
        "인코더는 다음 그림처럼 `SublayeConnection`, `EncoderLayer`, `Encoder`로 구성됩니다. 트랜스포머는 인코더와 디코더가 각각 N번 반복됩니다. 여기서 N번 반복되는 단위를 `EncoderLayer`로 구현하였고 이를 N번 복사하여 가지고 있는 클래스가 `Encoder`가 됩니다. `EncoderLayer`내부에는 `SublayerConnection`이 있게 됩니다. 각 클래스가 좀 복잡할 정도로 조각조각 나있다는 느낌이 드는데 전체적인 구조는 다음 그림과 같습니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1a_MqSdbp9Ngjhkl6cNAUICb5tuDfBVjk)\n",
        "\n",
        "\n",
        "그림은 보면 인코더는 `SublayerConnection`을 두 개, 디코더는 세 개 가지는 것을 알 수 있습니다.\n",
        "\n",
        " 먼저 가장 기본 단위인 `SublayerConnection`부터 알아봅니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wGmkKSRLzreJ"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "\n",
        "        # (어텐션 or ff)-[드랍아웃]-Add and Norm으로 바꿨음\n",
        "        # 드랍아웃은 논문에 sublayer에 썼다고 나와 있음\n",
        "        # page 7        \n",
        "        # We apply dropout [33] to the output of each sub-layer, \n",
        "        # before it is added to the sub-layer input and normalized. \n",
        "        # 층구성이 바뀌어서 아래쪽 hyperparam. warmup을 좀 키워야 됨\n",
        "        # return x + self.dropout(sublayer(self.norm(x)))\n",
        "        return self.norm(x + self.dropout(sublayer(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXnnsxlauzzB"
      },
      "source": [
        "`SublayerConnection`은 내부적으로 `LayerNorm`과 `Dropout`만 가지고 있습니다. 포워들 할 때 외부에서 `sublayer`를 넘겨받아 이 `sublayer`를 포워드 시키고 내부에 있는 드롭아웃과 정규화를 적용하는 식으로 동작합니다. \n",
        "\n",
        "그래서 `sublayer`가 어텐션 레이어이면 위 그림 아래쪽 회식 박스에 해당하고 피드 포워드 레이어이면 윗쪽 회색 박스에 해당하게 되겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LQf0QJhXMV0"
      },
      "source": [
        "`EncoderLayer`는 `SublayerConnection`을 두 개 가지고 있습니다. 그리고 이 위 그림에서 확인할 수 있듯이 `SublayerConnection` 두 개는 각각 (어텐션 레이어, Add & Norm)과 (피드포워드 레이어, Add & Norm)으로 구성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "a3l2Ya1Bzs7U"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        \n",
        "        # self_attn: MultiHeadedAttention\n",
        "        self.self_attn = self_attn\n",
        "\n",
        "        # feed_forward: PositionwiseFeedForward\n",
        "        self.feed_forward = feed_forward \n",
        "        \n",
        "        # SublayerConnection이 2개\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2) \n",
        "        self.size = size # d_model\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        # 1. 어텐션으로 람다 함수를 만들어서 sublayer[0]에 x와 함께 전달\n",
        "        #    이때 어텐션 \n",
        "        # 2. sublayer[0]가 포워드 되면서\n",
        "        # 3. 레이어노멀 하고\n",
        "        # 4. 인자로 넘긴 람다 함수가 sublayer로 돌면서 어텐션하고\n",
        "        # 5. 드랍아웃하고 \n",
        "        # 6. x와 더해져서 리턴 \n",
        "        # 그냥 self_atten을 바로 넘기지 않는 이유는 self_attn에 x, mask를 같이 받기 때문에\n",
        "        # x만 인자로 만들려고\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        # 원래 코드는 이 시점에서 \n",
        "        # x는 레이어놈-어텐션-드랍아웃-스킵커넥션 까지 됨\n",
        "        # 하지만 위 SublayerConnection.forward에서 순서를 원래 논문 순서로 맞춰서\n",
        "        # 어테션-드랍아웃-스킵커넥션-레이어놈 이 되었음\n",
        "        \n",
        "        # 7. 위와 마찬가지로 \n",
        "        # 원 코드 순서는 레이어놈-ff-드랍아웃-스킵커넥션\n",
        "        # 코드를 고쳐서 여기서도 \n",
        "        # ff-드랍아웃-스킵커넥션-레이어놈\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5bU41I3XYAZ"
      },
      "source": [
        "`EncoderLayer`는 내부적으로 `MultiHeadedAttention`과 `PositionwiseFeedForward`을 가지고 있습니다. 그리고 `SublayerConnection`을 두개 가지고 이 `SublayerConnection` 포워드 시킬 때 `layer`인자로 `MultiHeadedAttention`과 `PositionwiseFeedForward`을 각각 전달하게 됩니다. 그림을 보면 그림과 동일하게 구현되어 있음을 알 수 있습니다.\n",
        "\n",
        "이제 마지막으로 `Encoder`입니다. `Encoder`는 생성자로 넘어오는 `EncoderLayer`를 N개 복사하고 포워드시 각각을 포워드 시키는 간단한 구조로 작성되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xZR1YxShzoJc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # make_model()함수에서 아래처럼 생성될 예정\n",
        "        # Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "        # 여기 layer는 EncoderLayer\n",
        "        self.layers = clones(layer, N) \n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        # self.layers에는 EncoderLayer 여섯 개가 순차적으로 있음\n",
        "        # 이 함수는 다음처럼 호출됨\n",
        "        # EncoderDecoder.encode(\n",
        "        #     self.encoder(self.src_embed(src), src_mask)\n",
        "        # )\n",
        "        # src_mask: (nbatches, 1, n_seq_src)\n",
        "        \n",
        "        for layer in self.layers: \n",
        "            # 여기 layer는 EncoderLayer고 EncoderLayer를 포워드 시킨다.\n",
        "            # EncoderLayer포워드는 위 EncoderLayer 주석참고\n",
        "            x = layer(x, mask) \n",
        "        \n",
        "        # 논문 그림 구조와 맞추기 위해 SublayerConnection에서 norm하고\n",
        "        # 여기선 안하는 것으로 바꿈\n",
        "        # return self.norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNN5Hxlotn0Y"
      },
      "source": [
        "### Positionwise Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Wg2ZuCqpz-bD"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwaq4h6cj7S7"
      },
      "source": [
        "인코더에서 $(n_{\\text{seq}}, d_{\\text{model}})$ 크기를 가지는 텐서를 두번 Linear 층에 통과 시키게 되는데 이를 positionwise feed forward라고 이야기했습니다. 위 구조 그림에서 하늘색으로 표시된 부분입니다. 코드를 보면 가중치 행렬을 두번 곱하게 되는데 각 곱에 대해서 크기는 다음처럼 변하게 됩니다.\n",
        "\n",
        "$(n_{\\text{seq}}, d_{\\text{model}}) \\to (n_{\\text{seq}}, d_{\\text{ff}}) \\to (n_{\\text{seq}}, d_{\\text{model}})$\n",
        "\n",
        "이 변환은 $n_{\\text{seq}}$의 각 자리에 해당하는 토큰 표현 벡터를 $d_{\\text{model}}$에서 $d_{\\text{ff}}$로 변환했다 다시 $d_{\\text{model}}$차원으로 되돌리는 것입니다. positionwise라고 이름지은 이유는 각 토큰 표현에 해당하는 n_seq개 벡터가 각각 $d_{\\text{ff}}$로 변환되었다 다시 $d_{\\text{model}}$ 크기로 돌아오기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkzmc8hxYvNQ"
      },
      "source": [
        "이렇게 인코더에 대해서 모두 알아봤습니다. 인코더를 잘 이해하면 디코더를 80% 이상 이해한것입니다. 디코더에서는 추가로 고려해야할 사항을 중심으로 이야기 하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9pXBAPMzzfB"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH77gBTLqj_N"
      },
      "source": [
        "### 디코더에 적용되는 마스크\n",
        "\n",
        "디코더에도 셀프 어텐션이 적용되는데 이 때는 이미 생성된 토큰 끼리만 어텐션 해야 합니다. 아직 생기지도 않은<sup>&#8224;</sup> 토큰과 어텐션할 수 없기 때문입니다.  seq2seq 모델처럼 시간순으로 타겟 토큰을 입력한다면 이는 자연스럽게 해결되지만 트랜스포머는 학습시 디코더에 모든 정답 토큰이 입력되므로 아직 어텐션 하지 않아아 될 부분을 지워내야 합니다. 그렇게 하기 위해 디코더에 입력되는 정답 시퀀스 길이를 행과 열로 가지는 마스크 행렬을 만드는데 이 행렬은 하삼각 요소와 주대각선만 1이고 상삼각행렬은 0을 가지게 됩니다. \n",
        "\n",
        "\n",
        "---\n",
        "<sup>&#8224;</sup> \"아직 생기지도 않은\" 이란 표현에서 뭔가 혼란스러움을 느꼈다면 충분히 그럴 수 있습니다. 트랜스포머에는 모든 정답 토큰이 디코더로 한꺼번에 입력된다고 했는데 마치 시간 순으로 토큰이 디코딩 되는 듯한 \"아직 생기지도 않은\"이란 표현은 뭔가 어색하기 짝이 없습니다. 이런 이유 때문에 디코더쪽 마스킹을 이해하기 힘들어지는데 어떻게 표현하면 더 자연스러울지 잘 떠오르지 않아서 계속 고민중에 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tegWgGgDz3EU"
      },
      "outputs": [],
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRy3OXfI0Gzo"
      },
      "source": [
        "위 코드는 행과 열 수가 입력 토큰 수이고 상삼각 행렬이 모두 0인 행렬을 만드는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "FwfScgrCz4dP",
        "outputId": "d9c5991e-094a-41db-f2fa-6138a86be887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20, 20])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc673ec8210>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "sub_mask = subsequent_mask(20)\n",
        "print(sub_mask.shape)\n",
        "plt.imshow(sub_mask[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut7ZGkxI0ilu"
      },
      "source": [
        "이 마스크가 어떻게 적용되는지는 뒤에 상세히 예를 들어 설명하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loOPaKHueQsH"
      },
      "source": [
        "### DecoderLayer와 Decoder\n",
        "\n",
        "`DecoderLayer`와 `Decoder`의 구성은 인코더와 동일합니다. 한가지 다른 점은 `DecoderLayer`에는 어텐션이 두 개 있다는 점입니다. `forward()`함수를 보면 알 수 있듯이 셀프 어텐션과 인코더, 디코더 간의 크로스 어텐션입니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XqWPxChez1h2"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        # self_attn: MultiHeadedAttention\n",
        "        # src_attn: MultiHeadedAttention\n",
        "\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn # cross attention\n",
        "        self.feed_forward = feed_forward # positional ff\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        # 여기서 momory는 인코더로 부터 넘어온 인코딩 (nbatche, n_seq, d_model)\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        \n",
        "        # 크로스 어텐션 할 때 마스크는 src_mask(key_pad_mask)를 전달함!\n",
        "        # 이렇게 하면 입력에 존재하는 패딩 토큰이 쿼리로써 인코딩되어 온 것을 \n",
        "        # key 패딩으로 마스킹 할 수 있게 된다.\n",
        "        # (인코더 출력이 이제는 key로 작용하기 때문에!!!)\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        \n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsqlIIgdg27H"
      },
      "source": [
        "크로스 어텐션를 실행하는 코드를 보면 \n",
        "\n",
        "```python\n",
        "# x: 디코더에서 만든 (n_seq, d_mode) 텐서 (여기서 n_seq는 디코더로 입력되는 시퀀스의 길이)\n",
        "# m: 인코더에서 전달받은 (n_seq, d_model) 텐서 (여기서 n_seq는 인코더로 입력된 시퀀스의 길이)\n",
        "x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "```\n",
        "\n",
        "셀프 어텐션과 차이는 쿼리로 디코더의 입력이 인코딩된 것을 넘기고 키와 벨류는 인코더에서 입력받는 인코딩 정보를 넘긴다는 것입니다. 이렇게 해야 크로스 어텐션이 계산되겠죠. 또 마스크는 `src_mask`를 넘기고 있는 것을 확인할 수 있습니다. 이에 대한 자세한 설명은 아래 [실험용 데이터와 보조 코드 준비](#cell-id)절에서 모두 설명하도록 하겠습니다.\n",
        "\n",
        "아래는 `Decoder` 코드이고 특별히 언급할 점은 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EutLhZ9Ezu08"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        # layer: DecoderLayer\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        \n",
        "        # Encoder와 마찬가지로 norm 안함\n",
        "        # return self.norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fkVa3LgqArF"
      },
      "source": [
        "### 디코더 끝에 붙는 마지막 Linear 레이어\n",
        "\n",
        "디코더의 출력을 받아서 최종적으로 $(n_{\\text{seq}}, d_{\\text{model}})$을 $(n_{\\text{seq}}, \\text{vocab})$로 변환하는 피드 포워드 레이어를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4UeVvfIOzkoF"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k4OcZItp8aa"
      },
      "source": [
        "\n",
        "\n",
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IH3c-I1h9Ry"
      },
      "source": [
        "이상에서 만든 부품들로 트랜스포머를 조립합니다. `Encoder`, `Decoder`를 받아서 전체 모델을 생성하는 `EncoderDecoder` 클래스를 정의하고 이것을 객체로 생성하는 `make_model()` 함수를 정의합니다.\n",
        "\n",
        "아래 그림은 `make_model()` 함수가 조립하는 전체 구조를 주석과 함께 나타낸 것입니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1aQKj79KnivzAaQ2pVz6103kQ1hf4klBQ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "N26U07Vmze0L"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed # 인코더의 임베딩-위치인코딩\n",
        "        self.tgt_embed = tgt_embed # 디코더의 임베딩-위치인코딩\n",
        "        self.generator = generator\n",
        "\n",
        "    # 이 함수가 모델이 포워드 되는 엔트리 포인트!!!    \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(\n",
        "            self.encode(src, src_mask), src_mask,\n",
        "            tgt, tgt_mask\n",
        "        )\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrFUFtJ82r89"
      },
      "source": [
        "마지막으로 `Encoder`와 `Decoder`를 한번 더 감싸서 `EncoderDecoder` 클래스를 만듭니다. 이 클래스의 `forward`함수가 모델을 포워드시키는 엔트리 포인트가 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lKThOQWr0DGO"
      },
      "outputs": [],
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"\"\"\n",
        "        Helper: Construct a model from hyperparameters.\n",
        "        src_vocab: 입력을 임베딩할 때 사용하는 단어장 사이즈\n",
        "        tgt_vocab: 출력을 위한 출력쪽 단어장 사이즈\n",
        "        d_mode: 트랜스포머 인코더 디코더에서 사용되는 벡터의 크기\n",
        "        d_ff: feed foward층이 출력하는 벡터의 크기\n",
        "    \"\"\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        # 인코더를 만들기 위해 어텐선 레이어 하나와 피드포워드 레이어 하나가 필요합니다.\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        \n",
        "        # 디코더를 만들기 위해 어텐션 레이어 두개와 피드포워드 레이어 하나가 필요합니다.\n",
        "        # 두 어텐션 레이어 중 하나는 셀프 어텐션을 담당하고 하나는 크로스 어텐션을 담당합니다.\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "\n",
        "        # 인코더와 디코더 쪽 임베딩으로 직접 만든 토큰 임베딩과\n",
        "        # 포지션 인코딩을 순차적으로 수행하는 nn.Sequential                   \n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            # nn.init.xavier_uniform(p)\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_htqhWmeyKG"
      },
      "source": [
        "마지막으로 트랜스포머를 조립하는 `make_model` 함수입니다. 지금까지 내용을 충분히 이해했다면 여기서 모델을 만드는 코드가 한눈에 들어올 것입니다.\n",
        "\n",
        "`make_model` 함수의 각 부분이 그림의 어느 부분과 대응되는지 모두 그림으로 나타내었습니다.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1yOvDOXXr3hqHl_U9o2OfCXVDv8QEJbLE)\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1yW4H4_xBcxWQROHo61g4Pke7xXtdYKCH)\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1yXR2UYQr_LKLH12Q6lubZEhaw3EM78FM)\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1yXlxXW4zowQPwkd1r9TvAIuWSNTzenKD)\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1y_Lh9LUNZgXEfEhxrEsYN2NP3RtZBZ4k)\n",
        "\n",
        "이상없이 모델이 만들어지는지 테스트 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Aa2is5Bd0FGO"
      },
      "outputs": [],
      "source": [
        "# 무소식이 희소식! 아무 에러없이 실행되면 OK\n",
        "tmp_model = make_model(10, 10, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFTdWsxYDoZU"
      },
      "source": [
        "<a name=\"cell-id\"></a>\n",
        "## 실험용 데이터와 보조 코드 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_2JoQDfe4O1"
      },
      "source": [
        "이렇게 모델에 대한 코딩을 모두 마쳤고 이제 간단한 실험을 위한 코드를 알아보도록 하겠습니다. 먼저 `Batch`클래스입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sUD6rROm0Hf2"
      },
      "outputs": [],
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        # src: (nbatches, n_seq_src)\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2) # (nbatches, 1, n_seq_src)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = self.make_std_mask(self.trg, pad) # (nbatches, n_seq_trg, n_seq_trg)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum() # 패딩 토큰이 아닌 토큰 수\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2) # (nbatches, 1, n_seq_trg)\n",
        "        #tgt_mask = tgt_mask & Variable( subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data) )\n",
        "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
        "        return tgt_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui_CyOaWfAzH"
      },
      "source": [
        "`Batch`클래스는 입력 시퀀스와 타겟 시퀀스를 받아서 입력과 타겟의 마스크를 만들고 타겟을 디코더에 입력되는 타겟(`self.trg`)과 출력 타겟(`self.trg_y`)으로 분리합니다. 예를 들면 다음과 같습니다.\n",
        "\n",
        "- `src`: `I` `am` `a` `student.`\n",
        "\n",
        "- `trg`: `나는` `학생` `입니다.`\n",
        "\n",
        "    - `trg`: `나는` `학생`\n",
        "    - `trg_y`: `학생` `입니다.`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN-7kibnl2zp"
      },
      "source": [
        "다음으로 마스크를 만들게 되는데 마스크를 만드는 과정(`make_std_mask` 함수)은 코드만 봐서는 이해가 어려우므로 다음 예를 보면서 알아보겠습니다. \n",
        "\n",
        "이 글에서 어쩌면 가장 복잡하고 설명하기 지저분한 부분이니 잠시 쉬고 와서 읽도록 합시다. 😎"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR2XCJltHa1F"
      },
      "source": [
        "### Source mask (key pad mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ2_DBN-l4xZ"
      },
      "source": [
        "먼저 source mask에 대해서 알아봅시다. 아래 예에서는 시퀀스 길이 5인 샘플 두 개가 (2,5)인 행렬에 들어있는 상황입니다.\n",
        "\n",
        "예를 들어 각 샘플이 다음과 같다고 하겠습니다.\n",
        "\n",
        "*   `i,     love,  you,   [PAD],  [PAD]`\n",
        "*   `good,  job,  [PAD],  [PAD],  [PAD]`\n",
        "\n",
        "이렇게 구성된 샘플에 셀프어텐션을 하는 경우 [PAD] 토큰에는 어텐션이 되면 안될 것입니다. 예를 들어 'i'와 관계가 있는 토큰을 알아내는 것이 셀프어텐션인데 그 후보 키 토큰에 [PAD]가 들어가는 것은 별로 바람직하지 않습니다. 물론 [PAD]가 쿼리로 작동하는 것도 피해야 합니다.따라서 [PAD] 가 있는 위치를 마스킹할 수 있는 마스크를 만들어야 합니다. \n",
        "\n",
        "마스크를 만들고 이를 직접 그림으로 보이는 것은 것은 트랜스포머 모델을 실제로 포워드 시켜야 하는 과정을 거쳐야 해서 매우 귀찮은 작업이지만 하나 하나 따라 가보도록 하겠습니다.😵\n",
        "\n",
        " 단 문제를 단순화 하고 그림을 그리기 위해 헤드 하나를 가정합니다. \n",
        "우선 모델과 관련된 변수를 적당히 세팅합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eT9KxTQaxhlI"
      },
      "outputs": [],
      "source": [
        "model_const = {'dv':3, 'dk':3, 'h':1, 'd_model':7, 'src_len':5, 'target_len':6}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdeZzzmpBfMo",
        "outputId": "8fa615f4-3142-4970-f6fb-ef4000425a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src tensor([[ 9, 21, 89,  0,  0],\n",
            "        [92, 82,  0,  0,  0]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "pad = 0\n",
        "# 길이 7인 샘플 두 개, src:(2,5)\n",
        "# 1번 샘플은 길이 5, 패딩 2, 예를 들어 i     love  you   [PAD]  [PAD]  \n",
        "# 2번 샘플은 길이 2, 패딩 3, 예를 들어 good  job  [PAD]  [PAD]  [PAD]\n",
        "# 임의 숫자를 토큰 번호로 가정하고 randint를 사용\n",
        "src = torch.from_numpy(np.random.randint(1, 100, size=(2, model_const['src_len'])))\n",
        "paddings = torch.LongTensor([2, 3]).reshape(-1,1) # 각 샘플당 패딩 개수\n",
        "pad_idx = src.shape[1] - paddings\n",
        "col_idx = torch.arange(src.shape[1]).reshape(1,-1)\n",
        "# src = src * ~(paddings > col_idx)\n",
        "src = src * (pad_idx > col_idx)\n",
        "\n",
        "# 마지막 토큰들은 PAD토큰이 되었음\n",
        "print('src', src, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQPR8n5CBgnx"
      },
      "source": [
        "이렇게 가상으로 만들어진 src에서 0인 자리를 `False`, 0이 아닌 자리를 `True`로 가지는 마스크를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VHv2zCgBrlS",
        "outputId": "53a9bb13-0cde-4348-bcbc-8ac606c51526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "(src != pad).shape torch.Size([2, 5])\n",
            "tensor([[ True,  True,  True, False, False],\n",
            "        [ True,  True, False, False, False]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# (src != pad): (2,5)\n",
        "# 패딩 토큰자리만 False인 src와 모양이 같은 마스크\n",
        "print('\\n(src != pad).shape', (src != pad).shape)\n",
        "print(src != pad, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxsUaK5kB2U0"
      },
      "source": [
        "이제 멀티헤드 어텐션에 마스크를 적용하기 위해 중간 차원을 하나 더 늘립니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGoHTIc6B688",
        "outputId": "df943ad2-6c52-4cda-bea7-209e4bc1b695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "(src != pad).unsqueeze(-2).shape: torch.Size([2, 1, 5])\n",
            "tensor([[[ True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True,  True, False, False, False]]])\n"
          ]
        }
      ],
      "source": [
        "src_mask_enc = (src != pad).unsqueeze(-2)\n",
        "print('\\n(src != pad).unsqueeze(-2).shape:', src_mask_enc.shape)\n",
        "print(src_mask_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bXmUJuNB8Vx"
      },
      "source": [
        "결과는 (2,5)인 마스크가 (2,1,5)가 되는데 마스크가 적용될 시점의 코드를 다시보면 \n",
        "\n",
        "```python\n",
        "# MultiHeadedAttention.forward()에서\n",
        "    # Same mask applied to all h heads.\n",
        "    mask = mask.unsqueeze(1)\n",
        "    ...\n",
        "    x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "\n",
        "# attention()에서\n",
        "    # scores: (nbatches, h, n_seq, n_seq)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "```\n",
        "\n",
        "위 코드에서 `MultiHeadedAttention.forward()`를 거치면서 전달된 `src_mask`는 (2,1,5)가 (2,1,1,5)로 되며 이를 `(nbatches, h, n_seq, n_seq)`인 `score`에 적용하면 결국 h개 헤드에 행 방향으로만 마스크가 주어지게 됩니다. \n",
        "\n",
        "마스킹을 해서 그림을 그려보면\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "R7aixZgtDFpi",
        "outputId": "4ebee464-afa5-464c-85a5-522c5fb7a76d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0.4552, 0.8593, 0.3595, 0.0000, 0.0000],\n",
            "          [0.6904, 0.3907, 0.6186, 0.0000, 0.0000],\n",
            "          [0.0594, 0.6754, 0.2968, 0.0000, 0.0000],\n",
            "          [0.9511, 0.9876, 0.8440, 0.0000, 0.0000],\n",
            "          [0.3031, 0.3097, 0.6799, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.1745, 0.5724, 0.0000, 0.0000, 0.0000],\n",
            "          [0.7191, 0.7956, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3726, 0.7145, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2081, 0.5856, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5442, 0.4622, 0.0000, 0.0000, 0.0000]]]])\n"
          ]
        }
      ],
      "source": [
        "# 샘플 두개 헤드 하나짜리 (5,5)인 어텐션 스코어\n",
        "encoder_self_attn_scores = torch.rand((2, 1, model_const['src_len'], \n",
        "                                       model_const['src_len']))\n",
        "src_mask_enc_reshape = src_mask_enc.unsqueeze(1)\n",
        "\n",
        "# -1e9로 마스킹을 하면 마스킹 되지 않는 부분이 모두 같은 색으로 \n",
        "# 나타나므로 여기서는 그냥 0으로 마스킹\n",
        "masked_encoder_self_attn_scores = encoder_self_attn_scores.masked_fill(\n",
        "    src_mask_enc_reshape == 0, 0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,5), nrows=1, ncols=2)\n",
        "\n",
        "ax[0].imshow(masked_encoder_self_attn_scores[0][0], cmap='BrBG')\n",
        "ax[0].set_xticks([0, 1, 2, 3, 4])\n",
        "ax[0].set_xticklabels(['i', 'love', 'you', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[0].set_xlabel('Key', fontsize=15)\n",
        "ax[0].set_yticks([0, 1, 2, 3, 4])\n",
        "ax[0].set_yticklabels(['i', 'love', 'you', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[0].set_ylabel('Query', fontsize=15)\n",
        "ax[0].set_title('Sample 1', fontsize=15)\n",
        "\n",
        "ax[1].imshow(masked_encoder_self_attn_scores[1][0], cmap='BrBG')\n",
        "ax[1].set_xticks([0, 1, 2, 3, 4])\n",
        "ax[1].set_xticklabels(['good', 'job', '[PAD]', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[1].set_xlabel('Key', fontsize=15)\n",
        "ax[1].set_yticks([0, 1, 2, 3, 4])\n",
        "ax[1].set_yticklabels(['good', 'job', '[PAD]', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[1].set_ylabel('Query', fontsize=15)\n",
        "ax[1].set_title('Sample 2', fontsize=15)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(masked_encoder_self_attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLkGnGLkIMUb"
      },
      "source": [
        "[PAD]에 해당하는 열만 마스킹되었습니다. 어텐션 스코어 행렬에서 행은 쿼리에, 열은 키에 해당하는데 이렇게 마스킹되는 것은 결국 키만 마스킹 했다는 의미입니다. 다시말해  i, love, you, [PAD], [PAD]라는 단어 다섯개를 쿼리로 요청하였고 그에 대한 응답으로 [PAD] 두개는 마스킹하고 i, love, you만 키가 되어 요청된 쿼리와 가중치가 계산된 것입니다.\n",
        "\n",
        "어차피 필요없는 [PAD]를 왜 쿼리로 요청했는지 이상할 수 있습니다. 그 이유는 이렇게 키만 마스킹된 인코딩 정보가 디코더로 넘어가 크로스 어텐션될 때는 다시 키로 작용하게 되는데 그때 키가 $K^T$로 전치 되면서 다시 한번 마스킹을 적용하면 마스킹 되지 않았던 쿼리에 대한 [PAD]도 마스킹되어 사라지기 때문입니다. \n",
        "\n",
        "이런 이유 때문에 이 소스코드에서 `src_mask`가 PyTorch에서는 `key_padding_mask`라는 이름으로 제공됩니다.\n",
        "\n",
        "> key_padding_mask: If specified, a mask of shape (N,S) indicating which elements within key to ignore for the purpose of attention (i.e. treat as  padding”). \n",
        "\n",
        "해당 과정은 설명의 마지막 부분에서 구체적인 예를 들어 직접 코드로 확인하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqBQFZqwJl2X"
      },
      "source": [
        "### Target mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwx0zM31Kvw5"
      },
      "source": [
        "다음은 타겟쪽 마스크를 알아보겠습니다. 타겟 마스크도 먼저 [PAD] 위치를 마스킹 합니다. 이 과정은 앞서 알아본 `src_mask`와 동일합니다. 타겟 데이터는 소스가 번역된 다음 문장으로 가정합니다.\n",
        "\n",
        "\n",
        "*   `나는,  당신을,  사랑,   합니다, [PAD], [PAD], [PAD]`\n",
        "*   `잘,    했어,    [PAD],  [PAD],  [PAD], [PAD], [PAD]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp5FhIEkMMAS",
        "outputId": "556c8ebe-58e7-4449-a08b-7077a7517cfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trg tensor([[13, 22, 25, 50,  0,  0],\n",
            "        [99, 92,  0,  0,  0,  0]]) \n",
            "\n",
            "(trg != pad).shape torch.Size([2, 6])\n",
            "tensor([[ True,  True,  True,  True, False, False],\n",
            "        [ True,  True, False, False, False, False]]) \n",
            "\n",
            "(trg != pad).unsqueeze(-2).shape torch.Size([2, 1, 6])\n",
            "tensor([[[ True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True,  True, False, False, False, False]]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 디코더에 입력될 타겟을 준비한다. \n",
        "# shape: (2,6)\n",
        "# 임의 숫자를 토큰 번호로 가정하고 randint를 사용\n",
        "trg = torch.from_numpy(np.random.randint(1, 100, size=(2, model_const['target_len'])))\n",
        "paddings = torch.LongTensor([2, 4]).reshape(-1,1) # 각 샘플당 패딩 개수\n",
        "pad_idx = trg.shape[1] - paddings\n",
        "col_idx = torch.arange(trg.shape[1]).reshape(1,-1)\n",
        "# src = src * ~(paddings > col_idx)\n",
        "trg = trg * (pad_idx > col_idx)\n",
        "\n",
        "# 마지막 토큰들은 PAD토큰이 되었음\n",
        "print('trg', trg, '\\n')\n",
        "\n",
        "# src때와 마찬가지로 패딩 토큰자리만 False인 trg와 모양이 같은 마스크\n",
        "# shape: (2,6)\n",
        "print('(trg != pad).shape', (trg != pad).shape)\n",
        "print(trg != pad, '\\n')\n",
        "\n",
        "# src_mask때와 마찬가지로 차원을 늘린다.\n",
        "# subsequent_mask()에서 출력된 마스크와 브로드캐스팅된다.\n",
        "# (2,1,6)\n",
        "print('(trg != pad).unsqueeze(-2).shape', (trg != pad).unsqueeze(-2).shape)\n",
        "trg_mask = (trg != pad).unsqueeze(-2)\n",
        "print(trg_mask, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLungGfHMtVr"
      },
      "source": [
        "디코더 쪽에서 사용하는 타겟 마스크는 앞서 설명한 셀프 어텐션에 대한 마스킹도 함께 해야 하므로 `subsequnent_mask()`에서 생성된 마스크 행렬과 & 연산을 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpgjuuMx7mWv",
        "outputId": "2688d4f2-6231-4fe7-9075-a8e4ed847baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self_attn_mask.shape =>  torch.Size([1, 6, 6])\n",
            "tensor([[[ True, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True]]]) \n",
            "\n",
            "trg_mask_dec\n",
            "tensor([[[ True, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False]]])\n"
          ]
        }
      ],
      "source": [
        "# 입력 타켓으로 subsequent_mask 마스크 행렬을 만든다.\n",
        "# shape: (1, 6, 6)\n",
        "self_attn_mask = subsequent_mask(trg.size(-1)).type_as(trg_mask.data)\n",
        "print('self_attn_mask.shape => ', self_attn_mask.shape)\n",
        "print( self_attn_mask, '\\n')\n",
        "\n",
        "# 패딩 토큰 자리가 False인 trg_mask와 subsequent_mask()의 결과를 & 한다.\n",
        "# 다음 브로드캐스팅되는 과정이 있고 늘어나는 차원은 []로 표시했다.\n",
        "# (2,1,6) & (1, 6, 6) => (2, [6], 6) & ([2], 6, 6)\n",
        "trg_mask_dec = trg_mask & self_attn_mask\n",
        "print('trg_mask_dec')    \n",
        "print( trg_mask_dec )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSoA3EJdnYx_"
      },
      "source": [
        "위 코드의 주석을 읽고 마지막 출력 결과를 보면 패딩 토큰이 있는 행과 열은 모두 False이고 데이터가 있는 자리만 작게 하삼각 행렬이 True인 부분 행렬이 마스크로 만들어짐을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H158dtFsNhWq",
        "outputId": "5eaff94d-0250-4429-88d7-421b38e269f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45208 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45817 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51012 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46993 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54633 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45768 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51096 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54664 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45208 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45817 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51012 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46993 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54633 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45768 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51096 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54664 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 샘플 두개 헤드 하나짜리 (6,6)인 어텐션 스코어\n",
        "decoder_self_attn_scores = torch.rand((2, 1, 6, 6))\n",
        "trg_mask_dec_reshape = trg_mask_dec.unsqueeze(1)\n",
        "\n",
        "# -1e9로 마스킹을 하면 마스킹 되지 않는 부분이 모두 같은 색으로 \n",
        "# 나타나므로 여기서는 그냥 0으로 마스킹\n",
        "masked_decoder_self_attn_scores = decoder_self_attn_scores.masked_fill(trg_mask_dec_reshape == 0, 0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,5), nrows=1, ncols=2)\n",
        "\n",
        "ax[0].imshow(masked_decoder_self_attn_scores[0][0], cmap='BrBG')\n",
        "ax[0].set_xticks([0, 1, 2, 3, 4, 5])\n",
        "ax[0].set_xticklabels(['나는', '당신을', '사랑', '합니다', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[0].set_xlabel('Key', fontsize=15)\n",
        "ax[0].set_yticks([0, 1, 2, 3, 4, 5])\n",
        "ax[0].set_yticklabels(['나는', '당신을', '사랑', '합니다', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[0].set_ylabel('Query', fontsize=15)\n",
        "ax[0].set_title('Sample 1', fontsize=15)\n",
        "\n",
        "ax[1].imshow(masked_decoder_self_attn_scores[1][0], cmap='BrBG')\n",
        "ax[1].set_xticks([0, 1, 2, 3, 4, 5])\n",
        "ax[1].set_xticklabels(['잘', '했어', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[1].set_xlabel('Key', fontsize=15)\n",
        "ax[1].set_yticks([0, 1, 2, 3, 4, 5])\n",
        "ax[1].set_yticklabels(['잘', '했어', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[1].set_ylabel('Query', fontsize=15)\n",
        "ax[1].set_title('Sample 2', fontsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UZen0UQPYZE"
      },
      "source": [
        "마스킹된 그림을 보면 셀프 어텐션에 대한 마스킹 때문에 상삼각행렬에 해당하는 요소가 모두 마스킹되었고 타겟에 있는 [PAD]에 대해서 열방향으로 마스킹 된 것을 확인할 수 있습니다.\n",
        "\n",
        "이제 마지막으로 디코더에서 일어나는 크로스 어텐션에 대한 마스크를 확인하도록 하겠습니다. 이를 위해 적당히 (nbatches=2, head=1, dv=5) 크기를 가지는 벨류 텐서를 만들고 앞서 소스 마스크에서 만들어 놓은 `masked_encoder_self_attn_scores` 텐서와 곱해 헤드를 만듭니다.\n",
        "그 다음 임의로 초기화 된 $W^o$행렬과 곱해서 최종 인코딩을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtYp3hvB17pN",
        "outputId": "fb7ce230-debf-4d25-d5ed-fe4c9e798205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 5, 7])\n",
            "tensor([[[1.3157, 0.8231, 1.8169, 2.2399, 0.7467, 0.7838, 0.9017],\n",
            "         [1.0317, 0.6436, 1.4306, 1.7503, 0.5839, 0.5992, 0.7001],\n",
            "         [1.7735, 1.1149, 2.3128, 2.7675, 1.1070, 0.8919, 1.2505],\n",
            "         [2.4297, 1.5170, 3.2814, 3.9383, 1.4432, 1.2764, 1.6642],\n",
            "         [0.5630, 0.3511, 0.7676, 0.9250, 0.3292, 0.3033, 0.3835]],\n",
            "\n",
            "        [[0.7225, 0.4850, 0.8106, 1.1491, 0.5061, 0.5566, 0.6293],\n",
            "         [0.7500, 0.5038, 0.8479, 1.2112, 0.5195, 0.5941, 0.6536],\n",
            "         [0.7592, 0.5096, 0.8507, 1.2044, 0.5327, 0.5821, 0.6612],\n",
            "         [0.7580, 0.5088, 0.8508, 1.2065, 0.5306, 0.5848, 0.6602],\n",
            "         [0.3948, 0.2652, 0.4466, 0.6385, 0.2731, 0.3136, 0.3440]]])\n"
          ]
        }
      ],
      "source": [
        "V = torch.rand((2, model_const['h'], model_const['src_len'], model_const['dv']))\n",
        "# head가 하나 뿐이라 concat 할 필요없이 squeeze()합니다.\n",
        "head = torch.matmul(masked_encoder_self_attn_scores, V).squeeze() \n",
        "Wo = torch.rand(model_const['dv']*model_const['h'], model_const['d_model'])\n",
        "encoding_src = torch.matmul(head, Wo)\n",
        "\n",
        "print(encoding_src.shape)\n",
        "print(encoding_src)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvg7w2qh2DLM"
      },
      "source": [
        "이렇게 만들어진 인코딩에서 4행과 5행은 [PAD]가 인코딩된 정보임을 상기 합시다. 이제 디코더 쪽에서 타겟에 대한 인코딩을 동일한 방식으로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwQw-5dG2aSW",
        "outputId": "3599cdb3-0686-421a-ce00-ad64bb47cd2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 7])\n",
            "tensor([[[0.1092, 0.2553, 0.2908, 0.3739, 0.3702, 0.1855, 0.0366],\n",
            "         [0.6836, 1.0122, 1.1891, 1.4366, 1.6190, 0.8272, 0.2952],\n",
            "         [0.2467, 0.5165, 0.5903, 0.7477, 0.7585, 0.3868, 0.0951],\n",
            "         [1.0650, 1.6200, 1.7648, 1.9914, 2.1176, 1.4748, 0.8864],\n",
            "         [1.5561, 2.6366, 2.8497, 3.2805, 3.3495, 2.3318, 1.3270],\n",
            "         [0.8810, 1.3566, 1.5207, 1.7734, 1.9178, 1.1775, 0.5928]],\n",
            "\n",
            "        [[0.5224, 0.7097, 0.7051, 0.6843, 0.7026, 0.7583, 0.6666],\n",
            "         [0.3033, 0.5964, 0.5850, 0.6280, 0.5492, 0.5731, 0.4405],\n",
            "         [0.5860, 0.8012, 0.7957, 0.7740, 0.7920, 0.8543, 0.7493],\n",
            "         [0.2773, 0.5113, 0.5024, 0.5316, 0.4760, 0.4994, 0.3928],\n",
            "         [1.1601, 1.9902, 1.9602, 2.0378, 1.8777, 1.9823, 1.6005],\n",
            "         [0.7322, 1.0746, 1.0643, 1.0591, 1.0460, 1.1204, 0.9576]]])\n"
          ]
        }
      ],
      "source": [
        "V = torch.rand((2, 1, model_const['target_len'], model_const['dv']))\n",
        "head = torch.matmul(masked_decoder_self_attn_scores, V).squeeze()\n",
        "Wo = torch.rand(model_const['dv']*model_const['h'], model_const['d_model'])\n",
        "encoding_trg = torch.matmul(head, Wo)\n",
        "\n",
        "print(encoding_trg.shape)\n",
        "print(encoding_trg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab_t0pHC2oRj"
      },
      "source": [
        "이제 `encoding_src`를 키와 벨류로 사용하고 `encoding_trg`를 쿼리로 사용해서 어텐션합니다. 이때 어텐션 마스크는 소스 마스크인 `src_mask_enc_reshape`를 사용합니다.\n",
        "\n",
        "`encoding_src`와 `encoding_trg`가 `MultiHeadedAttention.forward()`로 입력되면 앞서 알아본과정을 통해 이 인코딩들도 멀티헤드 쿼리, 키, 벨류로 바뀌게 됩니다. 여기서는 이 과정을 거쳤다고 가정하고 바로 텐서를 (nbatches, h, n_seq, d_k)로 변환하여 사용하겠습니다.\n",
        "\n",
        "변환된 두 인코딩 텐서를 어텐션 연산하고 마스크를 적용합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw_GcUc070oN",
        "outputId": "262f7f98-5772-4eda-c3eb-5af9e8f52198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cross_QKt shape => torch.Size([2, 1, 6, 5])\n",
            "tensor([[[[ 2.1746,  1.7004,  2.8068,  3.9115,  0.9124],\n",
            "          [ 9.2343,  7.2198, 11.9659, 16.6396,  3.8791],\n",
            "          [ 4.4522,  3.4812,  5.7513,  8.0110,  1.8685],\n",
            "          [13.9381, 10.8922, 18.0558, 25.0925,  5.8496],\n",
            "          [22.2684, 17.4027, 28.8159, 40.0696,  9.3428],\n",
            "          [11.9004,  9.3017, 15.4144, 21.4298,  4.9960]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2766,  3.4272,  3.4387,  3.4388,  1.8053],\n",
            "          [ 2.5783,  2.6983,  2.7056,  2.7060,  1.4214],\n",
            "          [ 3.6942,  3.8640,  3.8769,  3.8770,  2.0354],\n",
            "          [ 2.2324,  2.3361,  2.3427,  2.3429,  1.2306],\n",
            "          [ 8.7947,  9.2023,  9.2293,  9.2301,  4.8475],\n",
            "          [ 4.8854,  5.1106,  5.1270,  5.1272,  2.6920]]]])\n",
            "tensor([[[[ 2.1746,  1.7004,  2.8068,  0.0000,  0.0000],\n",
            "          [ 9.2343,  7.2198, 11.9659,  0.0000,  0.0000],\n",
            "          [ 4.4522,  3.4812,  5.7513,  0.0000,  0.0000],\n",
            "          [13.9381, 10.8922, 18.0558,  0.0000,  0.0000],\n",
            "          [22.2684, 17.4027, 28.8159,  0.0000,  0.0000],\n",
            "          [11.9004,  9.3017, 15.4144,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2766,  3.4272,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.5783,  2.6983,  0.0000,  0.0000,  0.0000],\n",
            "          [ 3.6942,  3.8640,  0.0000,  0.0000,  0.0000],\n",
            "          [ 2.2324,  2.3361,  0.0000,  0.0000,  0.0000],\n",
            "          [ 8.7947,  9.2023,  0.0000,  0.0000,  0.0000],\n",
            "          [ 4.8854,  5.1106,  0.0000,  0.0000,  0.0000]]]])\n"
          ]
        }
      ],
      "source": [
        "multiheaded_encoding_src = encoding_src.unsqueeze(1)\n",
        "multiheaded_encoding_trg = encoding_trg.unsqueeze(1)\n",
        "\n",
        "cross_QKt = torch.matmul(multiheaded_encoding_trg, \n",
        "                         multiheaded_encoding_src.transpose(3,2))\n",
        "\n",
        "print('cross_QKt shape =>', cross_QKt.shape)\n",
        "print(cross_QKt)\n",
        "\n",
        "masked_cross_QKt = cross_QKt.masked_fill(src_mask_enc_reshape == 0, 0)\n",
        "print(masked_cross_QKt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od8KVLXs7-al"
      },
      "source": [
        "적용된 결과를 그려보면..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "vyMHxfrFPtRf",
        "outputId": "9d36a37c-0716-424a-bcf4-c94c51f10fec"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,5), nrows=1, ncols=2)\n",
        "\n",
        "ax[0].imshow(masked_cross_QKt[0][0], cmap='BrBG')\n",
        "ax[0].set_xticks([0, 1, 2, 3, 4])\n",
        "ax[0].set_xticklabels(['i', 'love', 'you', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[0].set_xlabel('Key', fontsize=15)\n",
        "ax[0].set_yticks([0, 1, 2, 3, 4, 5])\n",
        "ax[0].set_yticklabels(['나는', '당신을', '사랑', '합니다', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[0].set_ylabel('Query', fontsize=15)\n",
        "ax[0].set_title('Sample 1', fontsize=15)\n",
        "\n",
        "ax[1].imshow(masked_cross_QKt[1][0], cmap='BrBG')\n",
        "ax[1].set_xticks([0, 1, 2, 3, 4])\n",
        "ax[1].set_xticklabels(['go', 'job',  '[PAD]', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[1].set_xlabel('Key', fontsize=15)\n",
        "ax[1].set_yticks([0, 1, 2, 3, 4, 5])\n",
        "ax[1].set_yticklabels(['잘', '했어', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], fontsize=15)\n",
        "ax[1].set_ylabel('Query', fontsize=15)\n",
        "ax[1].set_title('Sample 2', fontsize=15)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6jm3HXj8DN-"
      },
      "source": [
        "드디어 😲 타겟의 쿼리 '나는', '당신을', '사랑', '합니다', '[PAD]', '[PAD]'가 소스쪽 키 'i', 'love', 'you'와 어텐션되는 것을 확인할 수 있습니다! 인코더에서 [PAD]가 인코딩되어 온 정보는 디코더쪽 크로스 어텐션에서 비로소 패딩되어 사라지는 것을 확인할 수 있습니다.\n",
        "\n",
        "<a name=\"cell-id-targetmask\"></a>\n",
        "그럼 쿼리로 쓰이는 타겟에 포함된 [PAD]는 언제 마스킹 될까요? 이 [PAD]는 로스를 계산할 때 마스킹되게 됩니다. 그 과정이 아래쪽 [Label Smoothing](#cell-id-labelsmoothing) 절에 나와 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZjtZBy_-Pah"
      },
      "source": [
        "## Coffe Break\n",
        "\n",
        "트랜스포머를 이해하는데 있어 매우 귀찮은 마스킹을 완전히 분석했습니다. 여기까지 읽고 내용을 머리에 그릴 수 있다면 이제 글을 더 읽지 말고 커피나 음료수를 마시면서 쉬도록 합시다. 나머지는 내일 읽어주셔도 됩니다. 😁"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJcN5-zm_MiL"
      },
      "source": [
        "## 장난감 데이터 생성함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoncHyDrbpUM"
      },
      "source": [
        "원문에서는 아래 함수처럼 입력과 출력이 동일한 무작위 숫자를 열개 생성하여 입력이 들어갔을 때 제대로 출력이 나오는지 체크합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9qffH6rN0lpl"
      },
      "outputs": [],
      "source": [
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1\n",
        "        src = data; src.requires_grad=False\n",
        "        tgt = data; tgt.requires_grad=False\n",
        "        yield Batch(src, tgt, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h78nlMpkd6Fl"
      },
      "source": [
        "입력과 동일한 출력이 나오는 예제는 너무 재미가 없으니 여기서는 이 예제를 살짝 바꿔서 길이 10짜리 무작위 숫자를 입력으로 사용하는 것은 똑같으나 출력은 5번부터 10번 숫자 즉 뒤쪽 절반에 +1을 한 것을 출력으로 설정하겠습니다.\n",
        "\n",
        "예를 들면 다음과 같습니다.\n",
        "\n",
        "```\n",
        "- 입력: 1 2 3 4 5 6 7 8  9 10\n",
        "- 출력: 1 2 3 4 5 7 8 9 10 11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKIKnFeQBFqV"
      },
      "source": [
        "간단하게 데이터를 생성하는 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5Ck0GGgOTSiA"
      },
      "outputs": [],
      "source": [
        "def data_gen2(V, batch, nbatches):\n",
        "    \"\"\"\n",
        "    무작위 데이터를 생성하는데\n",
        "    데이터 길이의 뒷 절반에 +1 한 것이 정답 벡터인 데이터를 생성한다.\n",
        "    V: 단어장에 단어수\n",
        "    batch: 한 미니배치에 샘플 수\n",
        "    nbatches: 한 에폭에 있는 미니배치 수\n",
        "    \"\"\"\n",
        "    for i in range(nbatches):\n",
        "        # 샘플하나당 시퀀스 길이는 10으로 고정\n",
        "        data = torch.from_numpy(np.random.randint(1, V-1, size=(batch, 10)))\n",
        "        data.requires_grad = False\n",
        "        data[:, 0] = 1\n",
        "        \n",
        "        src = data.clone() \n",
        "        tgt = data.clone()\n",
        "        # 뒤에 다섯개는 +1\n",
        "        tgt[:, V//2:] +=1 \n",
        "        \n",
        "        yield Batch(src, tgt, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf-ZS8TsThrC",
        "outputId": "870deb49-32f6-49c4-8bae-288773b68bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data.src:  tensor([[1, 6, 9, 2, 7, 4, 3, 4, 3, 1]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[1, 6, 9, 2, 7, 5, 4, 5, 4]])\n",
            "data.trg_y:  tensor([[6, 9, 2, 7, 5, 4, 5, 4, 2]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 8, 1, 9, 5, 7, 1, 7, 2, 4]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[1, 8, 1, 9, 5, 8, 2, 8, 3]])\n",
            "data.trg_y:  tensor([[8, 1, 9, 5, 8, 2, 8, 3, 5]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 1, 2, 2, 3, 4, 5, 2, 5, 4]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[1, 1, 2, 2, 3, 5, 6, 3, 6]])\n",
            "data.trg_y:  tensor([[1, 2, 2, 3, 5, 6, 3, 6, 5]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 4, 7, 5, 2, 8, 2, 2, 9, 3]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[ 1,  4,  7,  5,  2,  9,  3,  3, 10]])\n",
            "data.trg_y:  tensor([[ 4,  7,  5,  2,  9,  3,  3, 10,  4]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 3, 8, 8, 9, 9, 6, 1, 3, 7]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[ 1,  3,  8,  8,  9, 10,  7,  2,  4]])\n",
            "data.trg_y:  tensor([[ 3,  8,  8,  9, 10,  7,  2,  4,  8]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 1, 4, 5, 1, 7, 5, 9, 3, 2]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[ 1,  1,  4,  5,  1,  8,  6, 10,  4]])\n",
            "data.trg_y:  tensor([[ 1,  4,  5,  1,  8,  6, 10,  4,  3]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 5, 6, 3, 2, 9, 3, 5, 3, 8]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[ 1,  5,  6,  3,  2, 10,  4,  6,  4]])\n",
            "data.trg_y:  tensor([[ 5,  6,  3,  2, 10,  4,  6,  4,  9]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 1, 1, 6, 6, 8, 5, 9, 4, 1]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[ 1,  1,  1,  6,  6,  9,  6, 10,  5]])\n",
            "data.trg_y:  tensor([[ 1,  1,  6,  6,  9,  6, 10,  5,  2]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 5, 4, 4, 3, 4, 5, 2, 8, 4]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[1, 5, 4, 4, 3, 5, 6, 3, 9]])\n",
            "data.trg_y:  tensor([[5, 4, 4, 3, 5, 6, 3, 9, 5]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "data.src:  tensor([[1, 3, 5, 7, 2, 7, 3, 3, 1, 3]])\n",
            "data.src_mask:  tensor([[[True, True, True, True, True, True, True, True, True, True]]])\n",
            "data.trg:  tensor([[1, 3, 5, 7, 2, 8, 4, 4, 2]])\n",
            "data.trg_y:  tensor([[3, 5, 7, 2, 8, 4, 4, 2, 4]])\n",
            "data.trg_mask\n",
            " tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
          ]
        }
      ],
      "source": [
        "# data_gen2가 제대로 작동하는지 확인\n",
        "# 단어수: 11, 미니배치사이즈:1, 에폭당 미니배치:10\n",
        "for data in data_gen2(11, 1, 10) :\n",
        "    # 입력, teach_forcing의 입력, teach_forcing의 출력\n",
        "    print('data.src: ', data.src)\n",
        "    print('data.src_mask: ', data.src_mask)\n",
        "    print('data.trg: ', data.trg)\n",
        "    print('data.trg_y: ', data.trg_y)\n",
        "    print('data.trg_mask\\n', data.trg_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmQJGmokef8j"
      },
      "source": [
        "미니배치 내 샘플들을 최대 길이로 잘라주는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "egTWNxnQ0LRX"
      },
      "outputs": [],
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoTf84tRMNLq"
      },
      "source": [
        "## Learning Rate Warm Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uakqnXcbJbJM"
      },
      "source": [
        "$$\n",
        "lrate = d^{-0.5}_{\\text{model}}\\cdot \\min \\left({step\\_num}^{-0.5}, {step\\_num} \\cdot  {warmup\\_steps}^{-1.5} \\right)\n",
        "$$\n",
        "\n",
        "학습중 학습률을 위 식처럼 변경합니다. 식에서 $step\\_num$은 옵티마이저가 파라미터를 한번 업데이트할 때마다 1 증가하는 진행 스탭수를 나타냅니다. 학습 초기에는 $step\\_num$이 작기 때문에 다음과 같습니다.\n",
        "\n",
        "$$\n",
        "{step\\_num} \\cdot  {warmup\\_steps}^{-1.5} = \\min \\left({step\\_num}^{-0.5}, {step\\_num} \\cdot  {warmup\\_steps}^{-1.5} \\right)\n",
        "$$\n",
        "\n",
        "즉 $step\\_num$이 증가함에 따라 선형적으로 학습률이 증가합니다. $step\\_num$이 계속 증가하다가 $step\\_num =  warmup\\_steps$가 되면 $\\min$ 안에 두 항은 같아지게 되고 그 다음 스탭부터 \n",
        "\n",
        "$$\n",
        "{step\\_num}^{-0.5} = \\min \\left({step\\_num}^{-0.5}, {step\\_num} \\cdot  {warmup\\_steps}^{-1.5} \\right)\n",
        "$$\n",
        "\n",
        "가 되어 $step\\_num$의 제곱에 반비례하게 학습률이 천천히 줄어들게 됩니다. 코드를 보면 전체적으로 학습률의 크기를 조정하기 위해 모델 사이즈와 별도의 factor를 곱하는 것을 확인할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "s_LTh2Cm0NGb"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        # 초기에는 min( , )에서 뒷부분이 작동하여 step에 선형적으로 lr이 증가\n",
        "        # 그렇게 뒷 부분이 자꾸 커지다 step에 self.warmup과 같아지면\n",
        "        # 뒷부분이 step*step**(-1.5)=step**(-0.5)가 되고 \n",
        "        # step = self.warmup+1부터는 앞부분이 작아져서\n",
        "        # 어느 순간 step의 제곱근에 반비례하게 lr이 줄어듬\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) * \n",
        "             min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pbnvo2ye7xl"
      },
      "source": [
        "모델 사이즈 512, 256, 웜업스탭 4000, 8000인 경우에 대해서 학습률이 어떻게 변화하는지 그래프로 그리면 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "V-y_OLgv0PQQ",
        "outputId": "b07d2f04-0bf6-407b-a76e-3362fe3651c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc673d664d0>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "opts = [NoamOpt(512, 1, 4000, None), \n",
        "        NoamOpt(512, 1, 8000, None),\n",
        "        NoamOpt(256, 1, 4000, None)]\n",
        "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
        "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOLpNcrKSH_"
      },
      "source": [
        "<a name=\"cell-id-labelsmoothing\"></a>\n",
        "## Label Smoothing\n",
        "\n",
        "예측을 할 때 정답이라고 예측된 토큰에 거의 확률 1로 예측하는 것을 막기위해 레이블을 깍아주는 작업을 하면 예측 성능이 더 좋아지는 것으로 알려져 있습니다.\n",
        "\n",
        "아래 원문처럼 0, 1로 구성된 확률 분포를 정답 분포로 쓰지 않고 정답자리 확률을 0.8정도로 깍아내리고 깍아낸 0.2를 나머지 오답 자리에 고루 분배하여 정답 분포를 마치 '신뢰도'정도로 해석할 수 있게 하는 방식입니다.\n",
        "\n",
        "> We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has `confidence` of the correct word and the rest of the `smoothing` mass distributed throughout the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "_uYddYnl0R24"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        \n",
        "        # size_average and reduce are in the process of being deprecated, \n",
        "        # and in the meantime, specifying either of those two args will override reduction.\n",
        "        # self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        \n",
        "        # smoothing을 적용한 타겟과 로스를 구하므로 NLLLoss 대신 KLDivLoss 사용\n",
        "        self.criterion = nn.KLDivLoss(reduction='sum') # input: log-probabilities \n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        # x: model.generator에서 출력한 log_softmax 값\n",
        "\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2)) # 정답자리와 패딩자리 두자리 빼고\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        # 여기까지 스무딩 시켰고...\n",
        "        \n",
        "        # 패딩 토큰 위치는 확률을 0으로 지정\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        \n",
        "        # target이 패팅토큰 번호라면 그 데이터에 대해서는 로스를 구할 필요\n",
        "        # 없으므로 모든 확률분포자리를 0으로 만들어 버림\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            # index_fill_(dim, index, val): dim차원을 따라 index가 지정된 위치에 val을 채움\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "\n",
        "        # loss 계산\n",
        "        return self.criterion(x, true_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcTg7E9WhaFS"
      },
      "source": [
        "늘 그렇듯이 위 코드를 보면 또 이해가 쉽지 않습니다. 그래서 하나씩 뜯어서 알아보도록 하겠습니다. 간단한 예제를 위해 단어장 크기가 5라고 가정하겠습니다. `smoothing=0.4`로 두면 다음처럼 시작할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0RCnP1R-h-fj"
      },
      "outputs": [],
      "source": [
        "size = 5\n",
        "smoothing = 0.4\n",
        "# 정답자리가 가져갈 확률 원래는 1인데 (1-smoothing)으로 깍아버리고\n",
        "# 깍은 만큼(smoothing)을 오답자리에 동일하게 나눠 주는 작업을 시작\n",
        "confidence = 1- smoothing \n",
        "padding_idx = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofZSXUIGh_iG"
      },
      "source": [
        "총 세 개 샘플에 대해서 다음처럼 예측이 나왔다고 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5cgZHQnTiGmy"
      },
      "outputs": [],
      "source": [
        "# 예측은 2번, 2번, 2번 토큰으로 예측\n",
        "# 총 다섯개의 토큰이 있고 0번 토큰은 [PAD], 1~4번 토큰은 일반 토큰\n",
        "#              토큰위치 0    1    2    3   4  \n",
        "x = torch.FloatTensor([[0, 0.2, 0.7, 0.1,  0],\n",
        "                       [0, 0.2, 0.7, 0.1,  0], \n",
        "                       [0, 0.2, 0.7, 0.1,  0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAZUZHbhiHmi"
      },
      "source": [
        "세 개 샘플에 대해서 모두 2번 단어가 정답이라고 예측한 상황입니다. 그리고 정답 타겟은 (2, 0, 0)이라고 가정하겠습니다. 측 첫 샘플만 2번 단어가 정답이고 나머지 뒤 두개는 패딩 토큰인 상황입니다. 각 샘플에 대한 정답 분포르 만듭니다. 원래 정답분포는 다음과 같아야 하지만\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0 & 0& 1& 0& 0 \\\\\n",
        "1& 0& 0& 0& 0 \\\\\n",
        "1& 0& 0& 0& 0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "smoothing을 통해 어떻게 변하는지 확인해보기로 합시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMrGF-bzif03",
        "outputId": "0546bc3f-b1f3-4940-d6f6-b8e93c966016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],\n",
            "        [0.6000, 0.1333, 0.1333, 0.1333, 0.1333],\n",
            "        [0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])\n"
          ]
        }
      ],
      "source": [
        "# 타겟은 2, 0, 0\n",
        "target  = torch.LongTensor([2,0,0])\n",
        "true_dist = x.clone()\n",
        "\n",
        "# 정답1개, 패딩1개 해서 -2\n",
        "# 여기서 smoothing을 정답과 패딩 토큰 위치를 제외한 나머지 토큰들에게\n",
        "# 나눠준다.\n",
        "true_dist.fill_(smoothing / (size-2)) \n",
        "\n",
        "# 정답자리에는 confidence, 여기선 0.6을 대입\n",
        "#                  1번축을 따라\n",
        "#                     이 인덱스에 해당하는 위치에\n",
        "#                                               confidence값을 대입\n",
        "true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
        "print(true_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHYVZagonvcG"
      },
      "source": [
        "계산된 `true_dist`는 원래 정답분포가 0인 자리에 `(1-smoothing) / 3`값이 들어가고 1인 자리에 `smoothing`값이 들어간것을 확인할 수 있습니다.\n",
        "\n",
        "이제 나머지 작업을 수행합니다. 패딩 토큰에 0.6이라는 확률이 부여 되어 있는데 패딩 토큰은 예측할 일이 없으니 0으로 만듭니다. 그리고 정답이 패딩토큰인 샘플은 모든 자리를 0으로 만들어 버립니다. \n",
        "\n",
        "[Target mask] 절에서 최종적으로 마스킹된 결과에 디코더에서 입력된 [PAD] 토큰 위치는 마스킹하지 않고 그대로 둔 것이 기억 나시나요? 기억이 잘나지 않으면 [여기](#cell-id-targetmask)를 클릭해서 다시 돌아가봅시다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnMJSkfLVVm5",
        "outputId": "202ff165-a2c5-4dcb-a5c4-79c3e5abd762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
            "        [0.0000, 0.1333, 0.1333, 0.1333, 0.1333],\n",
            "        [0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
            "tensor([False,  True,  True])\n",
            "tensor([[1],\n",
            "        [2]])\n",
            "tensor([1, 2])\n",
            "tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
          ]
        }
      ],
      "source": [
        "# 예측 벡터의 길이는 단어장 단어 개수와 같은데 \n",
        "# 패딩 토큰은 예측할 일이 없으니 패딩 토큰 자리는 확률을 0으로 지정\n",
        "true_dist[:, padding_idx] = 0 \n",
        "print(true_dist)\n",
        "\n",
        "# target이 패딩되어 있어서 (타겟도 시퀀스니까 미니배치 안에서 패딩될 수 있음)\n",
        "# [2, padding_idx, padding_idx]처럼 target이 생겼는데\n",
        "# target에서 padding_idx가 나타나는 위치에 예측 벡터는 모조리 0으로 채움\n",
        "# 이렇게 하면 2번, 3번 데이터는 로스 계산에서 빠짐\n",
        "# torch.nonzero(..., as_tuple=False) \n",
        "# (default) returns a 2-D tensor where each row is the index for a nonzero value.\n",
        "print(target.data == padding_idx)\n",
        "mask = torch.nonzero(target.data == padding_idx) # 정답이 패딩인덱스와 같은 데이터 번호\n",
        "print(mask)\n",
        "print(mask.squeeze())\n",
        "\n",
        "# index_fill_(dim, index, val): dim차원을 따라 index가 지정된 위치에 val을 채움\n",
        "true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "print(true_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn2mlSOmKccL"
      },
      "source": [
        "마지막에 출력된 결과 텐서를 보면 1행만 smoothing된 확률분포가 들어있고 [PAD] 토큰에 해당하는 2, 3행은 모두 0으로 채워진 것을 볼 수 있습니다. 이렇게 해서 마스킹에 대한 세부 사항이 모두 정리가 되었습니다. 🥳\n",
        "\n",
        "위 `LabelSmoothing` 클래스는 이 코드들을 하나로 묶에 놓은 것입니다.  `LabelSmoothing`클래스를 이용해서 실제로 smoothing하는 예제가 아래 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "NnWsqjC20UyS",
        "outputId": "08d4a145-1262-4fda-e9c0-b7834766fa3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
            "        [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example of label smoothing.\n",
        "# 단어장 크기가 5인데 0번 단어는 패딩토큰\n",
        "crit1 = LabelSmoothing(5, 0, 0.0)\n",
        "crit2 = LabelSmoothing(5, 0, 0.4)\n",
        "\n",
        "# 예측은 2번, 2번, 2번 토큰으로 예측\n",
        "# 총 다섯개의 토큰이 있고 0번 토큰은 [PAD], 1~4번 토큰은 일반 토큰\n",
        "#                    토큰위치 0    1    2    3   4  \n",
        "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1,  0],\n",
        "                             [0, 0.2, 0.7, 0.1,  0], \n",
        "                             [0, 0.2, 0.7, 0.1,  0]])\n",
        "# 정답은 2번, 1번, 0번, 즉 마지막 단어는 패딩토큰이 정답이라면\n",
        "v1 = crit1(predict.log(), torch.LongTensor([2, 1, 0]))\n",
        "v2 = crit2(predict.log(), torch.LongTensor([2, 1, 0]))\n",
        "\n",
        "# Show the target distributions expected by the system.\n",
        "# 결과를 보면 패딩토큰 자리는 모두 0 (1번 열)\n",
        "# target에서 패딩된 단어에 해당하는 예측 3행은 모든 자리가 0\n",
        "fig, ax = plt.subplots(figsize=(10,5), nrows=1, ncols=2)\n",
        "\n",
        "ax[0].set_title(\"Distribution of target without smoothing\")\n",
        "print(crit1.true_dist)\n",
        "ax[0].imshow(crit1.true_dist)\n",
        "\n",
        "ax[1].set_title(\"Distribution of target with smoothing\")\n",
        "print(crit2.true_dist)\n",
        "ax[1].imshow(crit2.true_dist)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTVzkFiaKfPV"
      },
      "source": [
        "`LabelSmoothing` 클래스에서 정답 분포를 smoothing하고 나서 로스 까지 계산하므로 정답자리에 대한 확신이 점점 1에 가까워지면 로스가 어떻게 계산되는지 확인해볼 수 있습니다.\n",
        "\n",
        ">Label smoothing actually starts to penalize the model if it gets very confident about a given choice.\n",
        "\n",
        "아래 코드셀에서 `x`가 커지면 `x/d`가 점점 1에 가깝게 되고 그렇게 되면 로스 값이 조금씩 증가하는 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "mH9l5qAF0WSn",
        "outputId": "5867a552-90fb-4fbb-ea93-cd40211cceed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc673de3490>]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "crit = LabelSmoothing(5, 0, 0.1)\n",
        "def loss(x):\n",
        "    d = x + 3 * 1\n",
        "\n",
        "    predict = torch.FloatTensor([[0, x/d, 1/d, 1/d, 1/d]])\n",
        "    \n",
        "    return crit(predict.log(), torch.LongTensor([1]))#.data[0]\n",
        "\n",
        "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3cdO9M0HM4"
      },
      "source": [
        "## Loss 계산\n",
        "\n",
        "`LabelSmoothing`에서 smoothing 뿐만 아니라 `nn.KLDivLoss`를 사용해서 로스값을 계산합니다. 원래 정답분포는 정답자리만 1이고 나머지는 모두 0입니다. 이럴 때 주로 사용하는 손실함수는 `nn.NLLLoss`입니다. 하지만 `LabelSmoothing`이 정답분포를 부드럽게 깍았으므로 모든 오답자리에도 확률이 할당되어 있습니다. 이럴때 출력분포와 정답분포의 차이를 계산하기 위해 Kullback–Leibler divergence[[7](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)]를 사용합니다. `SimpleLossCompute`는 `model.generator`를 호출해서 모델의 출력값을 계산한다음 그 값을 `LabelSmoothing`에 넘겨 로스값을 받아오고 백워드까지 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SFQKwCOT1MLH"
      },
      "outputs": [],
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        # 여기서 generator는 model.generator\n",
        "        self.generator = generator\n",
        "        \n",
        "        # 여기서 criterion은 LabelSmoothing \n",
        "        self.criterion = criterion\n",
        "\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        # norm은 batch에서 토큰 수\n",
        "        # self.ntokens = (self.trg_y != pad).data.sum() # 패딩 토큰이 아닌 토큰 수\n",
        "        \n",
        "        x = self.generator(x)\n",
        "        \n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        # return loss.data[0] * norm\n",
        "        return loss * norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3trxirYgDG0"
      },
      "source": [
        "## 학습\n",
        "\n",
        "모든 설명이 끝이 났습니다! 이제 적당히 학습 함수를 정의하고 학습을 하면 됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ylQqmx3R0J3X"
      },
      "outputs": [],
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    # data_iter를 만들 때 설정한 nbatches 만큼 루프를 돈다. 즉 1에폭\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        \n",
        "        # 여기서 loss_compute()는 SimpleLossCompute 임\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        \n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d, Loss: %f, Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK_IZIRp1N0O",
        "outputId": "2da1f746-6831-4eae-c5eb-7d18b4f151bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 epoch train\n",
            "Epoch Step: 1, Loss: 3.354272, Tokens per Sec: 377.661591\n",
            "eval\n",
            "Epoch Step: 1, Loss: 2.303514, Tokens per Sec: 549.083984\n",
            "eval_loss: tensor(2.2935, grad_fn=<DivBackward0>) \n",
            "\n",
            "1 epoch train\n",
            "Epoch Step: 1, Loss: 2.412700, Tokens per Sec: 454.465576\n",
            "eval\n",
            "Epoch Step: 1, Loss: 2.070032, Tokens per Sec: 240.111206\n",
            "eval_loss: tensor(2.0553, grad_fn=<DivBackward0>) \n",
            "\n",
            "2 epoch train\n",
            "Epoch Step: 1, Loss: 2.190294, Tokens per Sec: 217.004211\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.947735, Tokens per Sec: 549.814148\n",
            "eval_loss: tensor(1.9390, grad_fn=<DivBackward0>) \n",
            "\n",
            "3 epoch train\n",
            "Epoch Step: 1, Loss: 2.088120, Tokens per Sec: 444.893890\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.924506, Tokens per Sec: 543.242249\n",
            "eval_loss: tensor(1.8564, grad_fn=<DivBackward0>) \n",
            "\n",
            "4 epoch train\n",
            "Epoch Step: 1, Loss: 2.090795, Tokens per Sec: 446.859558\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.757596, Tokens per Sec: 544.436584\n",
            "eval_loss: tensor(1.7573, grad_fn=<DivBackward0>) \n",
            "\n",
            "5 epoch train\n",
            "Epoch Step: 1, Loss: 2.011374, Tokens per Sec: 449.039307\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.759653, Tokens per Sec: 551.795410\n",
            "eval_loss: tensor(1.6950, grad_fn=<DivBackward0>) \n",
            "\n",
            "6 epoch train\n",
            "Epoch Step: 1, Loss: 1.823795, Tokens per Sec: 450.738098\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.556262, Tokens per Sec: 549.926147\n",
            "eval_loss: tensor(1.5922, grad_fn=<DivBackward0>) \n",
            "\n",
            "7 epoch train\n",
            "Epoch Step: 1, Loss: 1.759499, Tokens per Sec: 449.286041\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.546365, Tokens per Sec: 541.197388\n",
            "eval_loss: tensor(1.4369, grad_fn=<DivBackward0>) \n",
            "\n",
            "8 epoch train\n",
            "Epoch Step: 1, Loss: 1.543564, Tokens per Sec: 451.066803\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.297869, Tokens per Sec: 545.020203\n",
            "eval_loss: tensor(1.2885, grad_fn=<DivBackward0>) \n",
            "\n",
            "9 epoch train\n",
            "Epoch Step: 1, Loss: 1.553686, Tokens per Sec: 442.650024\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.226753, Tokens per Sec: 549.382019\n",
            "eval_loss: tensor(1.2364, grad_fn=<DivBackward0>) \n",
            "\n",
            "10 epoch train\n",
            "Epoch Step: 1, Loss: 1.418336, Tokens per Sec: 448.515015\n",
            "eval\n",
            "Epoch Step: 1, Loss: 1.003431, Tokens per Sec: 548.947815\n",
            "eval_loss: tensor(1.0272, grad_fn=<DivBackward0>) \n",
            "\n",
            "11 epoch train\n",
            "Epoch Step: 1, Loss: 1.267661, Tokens per Sec: 450.780975\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.867174, Tokens per Sec: 552.161682\n",
            "eval_loss: tensor(0.8513, grad_fn=<DivBackward0>) \n",
            "\n",
            "12 epoch train\n",
            "Epoch Step: 1, Loss: 1.112552, Tokens per Sec: 442.590088\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.672556, Tokens per Sec: 545.049438\n",
            "eval_loss: tensor(0.6827, grad_fn=<DivBackward0>) \n",
            "\n",
            "13 epoch train\n",
            "Epoch Step: 1, Loss: 1.021866, Tokens per Sec: 438.177673\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.552568, Tokens per Sec: 547.756836\n",
            "eval_loss: tensor(0.4938, grad_fn=<DivBackward0>) \n",
            "\n",
            "14 epoch train\n",
            "Epoch Step: 1, Loss: 0.781048, Tokens per Sec: 445.803223\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.424077, Tokens per Sec: 540.819946\n",
            "eval_loss: tensor(0.4285, grad_fn=<DivBackward0>) \n",
            "\n",
            "15 epoch train\n",
            "Epoch Step: 1, Loss: 1.013065, Tokens per Sec: 446.153259\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.420987, Tokens per Sec: 550.075317\n",
            "eval_loss: tensor(0.4657, grad_fn=<DivBackward0>) \n",
            "\n",
            "16 epoch train\n",
            "Epoch Step: 1, Loss: 0.686301, Tokens per Sec: 449.449463\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.313754, Tokens per Sec: 548.636536\n",
            "eval_loss: tensor(0.3456, grad_fn=<DivBackward0>) \n",
            "\n",
            "17 epoch train\n",
            "Epoch Step: 1, Loss: 0.534895, Tokens per Sec: 443.117310\n",
            "eval\n",
            "Epoch Step: 1, Loss: 0.236053, Tokens per Sec: 550.462341\n",
            "eval_loss: tensor(0.2740, grad_fn=<DivBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the simple copy task.\n",
        "V = 11\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, V, N=2)\n",
        "#                   model_size, factor, warmup, optimizer\n",
        "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 1200,\n",
        "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "for epoch in range(18):\n",
        "    model.train()\n",
        "    print(f'{epoch} epoch train')\n",
        "    run_epoch(data_gen2(V, 30, 20), model, # 미니배치에 샘플 30개씩 20배치가 한에폭 \n",
        "              SimpleLossCompute(model.generator, criterion, model_opt))\n",
        "    print('eval')\n",
        "    model.eval()\n",
        "    eval_loss = run_epoch(data_gen2(V, 30, 5), model, # 미니배치에 샘플 30개씩 5배치가 한 에폭\n",
        "                    SimpleLossCompute(model.generator, criterion, None))\n",
        "    print('eval_loss:', eval_loss,'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUrupippgGIZ"
      },
      "source": [
        "## 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T19efGH6gKZI"
      },
      "source": [
        "트랜스포머를 학습 시킬 때 디코더에 정답이 모두 입력되고 출력도 한번에 나오게 됩니다. 하지만 학습이 끝나고 예측을 할 때는 그렇게 하지 않고 다음과 같은 순서로 하게 됩니다.\n",
        "\n",
        "1. 디코더에 [START] 토큰을 입력한다.\n",
        "\n",
        "2. [START] 토큰 다음에 나올 토큰을 트랜스포머가 예측한다.\n",
        "\n",
        "3. 예측된 [TKN1]을 붙여서 [START], [TKN1]을 디코더에 입력한다.\n",
        "\n",
        "4. 이런 식으로 계속 진행한다.\n",
        "\n",
        "이 과정에서 트랜스포머의 예측은 (nbatches, n_seq, vocab) 사이즈로 나오게 되는데 현재 스텝의 예측을 결정하기 위해 (nbatches, -1, vocab)에서 확률값이 가장 큰것을 고르게 됩니다. 이렇게 각 타임 스탭에서 적합한 단어를 고르는 전략을 탐욕 탐색greedy search라고 합니다. 하지만 이런 전략이 전체 시퀀스에 대해서 꼭 좋은 것은 아니라 빔서치같은 더 복잡한 방법을 사용하기도 합니다. 여기서는 가장 간단한 방법인 탐욕 탐색을 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JsKP-ZSX1QI8"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    # 시작은 [START]로 시작한다.\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    \n",
        "    # 생성할 시퀀스의 최대 길이만큰 순환하면서\n",
        "    for i in range(max_len-1):\n",
        "        print('ys.shape:', ys.shape)\n",
        "        out = model.decode(\n",
        "                    memory, src_mask, ys, \n",
        "                    subsequent_mask(ys.size(1)).type_as(src.data)\n",
        "                )\n",
        "        print('out.shape:', out.shape)\n",
        "        print('out[:, -1].shape:', out[:, -1].shape)\n",
        "        \n",
        "        # 마지막 타임스탭의 결과를 단어들로 바꾼다.\n",
        "        prob = model.generator(out[:, -1])\n",
        "        print('prob.shape:', prob.shape)\n",
        "        \n",
        "        # 가장 확률이 높은 단어를 선택한다.\n",
        "\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "\n",
        "        # 예측된 단어를 추가하고 루프 처음으로 돌아가 다시 ys를 디코더로 입력한다.\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        print('\\n')\n",
        "    return ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k0G9Z7W3hc6",
        "outputId": "ced1bb00-d9b2-4eef-a69f-7d2bff941d2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(11, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(11, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=512, out_features=11, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Za-Ur6fz1J"
      },
      "source": [
        "아무 샘플 데이터나 무작위로 만들어 모델에 입력하고 입력 숫자들 중 뒤 다섯개가 1큰 숫자로 출력되는지 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qw9mZ013v0Q",
        "outputId": "c6a9138a-094c-41d4-96f9-916a729d2ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ys.shape: torch.Size([1, 1])\n",
            "out.shape: torch.Size([1, 1, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 2])\n",
            "out.shape: torch.Size([1, 2, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 3])\n",
            "out.shape: torch.Size([1, 3, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 4])\n",
            "out.shape: torch.Size([1, 4, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 5])\n",
            "out.shape: torch.Size([1, 5, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 6])\n",
            "out.shape: torch.Size([1, 6, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 7])\n",
            "out.shape: torch.Size([1, 7, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 8])\n",
            "out.shape: torch.Size([1, 8, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "ys.shape: torch.Size([1, 9])\n",
            "out.shape: torch.Size([1, 9, 512])\n",
            "out[:, -1].shape: torch.Size([1, 512])\n",
            "prob.shape: torch.Size([1, 11])\n",
            "\n",
            "\n",
            "tensor([[ 1,  3,  4,  5,  6,  9,  8,  3, 10,  8]])\n"
          ]
        }
      ],
      "source": [
        "src = torch.LongTensor([[1, 3, 4, 5, 6,   8, 7, 2, 9,  7]])\n",
        "# 정답                   1, 3, 4, 5, 6,   9, 8, 3, 10, 8\n",
        "                         \n",
        "\n",
        "src_mask = torch.ones(1, 1, 10)\n",
        "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw06A7PTf7PR"
      },
      "source": [
        "예상처럼 잘 출력되는 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX-2_n7pf-nY"
      },
      "source": [
        "## 마무리\n",
        "\n",
        "처음 별것 아니라고 하고 시작한 글이 적고보니 엄청 복잡해진 듯한 느낌이 듭니다. 작성하는데 시간도 꽤 많이 걸린 듯 합니다. 부디 트랜스포머를 처음 공부하는 누군가에게 도움이 되길 바랍니다. 혹시 글에 대해서 이해하기 어려운 부분이나 수정이 필요한 부분이 있다면 언제든 연락주세요. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "annotated_transformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
