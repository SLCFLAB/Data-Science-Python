{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad4f6f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ref: ì„œìš¸ëŒ€í•™êµ ì‹ íš¨í•„ êµìˆ˜ë‹˜ì˜ í…ìŠ¤íŠ¸ ë° ìì—°ì–´ ë¹…ë°ì´í„° ë¶„ì„ ë°©ë²•ë¡  2021-2í•™ê¸° ê°•ì˜ì—ì„œ ë°œì·Œëœ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "https://hpshin.github.io/NaturalLanguageBigDataAnalysis/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70cdc6",
   "metadata": {},
   "source": [
    "# HuggingFaceğŸ¤— [Documentation](https://huggingface.co/transformers/quicktour.html)\n",
    "\n",
    "![HuggingFace Image](https://monologg.kr/images/2020-05-01-transformers-porting/thumbnail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd173818",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quick tour\n",
    "\n",
    "## What transformers can do\n",
    "- __Sentiment analysis__: is a text positive or negative?\n",
    "- __Text generation__: provide a prompt and the model will generate what follows.\n",
    "- __Name entity recognition (NER)__: in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
    "- __Question answering__: provide the model with some context and a question, extract the answer from the context.\n",
    "- __Filling masked text__: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\n",
    "- __Summarization__: generate a summary of a long text.\n",
    "- __Translation__: translate a text in another language.\n",
    "- __Feature extraction__: return a tensor representation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b5743",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How? Pipelining\n",
    "![Pipelining](https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d9672",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting started\n",
    "Installing transformers (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac11b78f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ba4f9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Importing transformers (hugging face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40e294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8695712",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example (Sentiment Analysis)\n",
    "- Lets classify the following sentence\n",
    "    - \"We are very happy to show you the ğŸ¤— Transformers library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f407b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3cf336e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are very happy to show you the ğŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8b4ae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- but how do you classify more than one sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158d5ff4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758},\n",
       " {'label': 'NEGATIVE', 'score': 0.5308598279953003}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\n",
    "    \"We are very happy to show you the ğŸ¤— Transformers library.\",\n",
    "    \"We hope you don't hate it.\"\n",
    "]\n",
    "classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4e937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Just now we used [DistillBert](https://huggingface.co/transformers/model_doc/distilbert.html)\n",
    "- To be more specific \"distilbert-base-uncased-finetuned-sst-2-english\" \n",
    "    - Model: DistillBert\n",
    "    - Model size: base\n",
    "    - Input: lowercased input\n",
    "    - Finetuned: SST (Stanford Sentiment Treebank)\n",
    "    - Language: English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f832259",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DistillBert (Distilled BERT)\n",
    "- Advantages\n",
    "    - 40% less parameters than bert-base-uncased\n",
    "    - 60% faster while preserving over 95% of BERTâ€™s performances\n",
    "![ModelvsParam Image](https://4.bp.blogspot.com/-v0xrp7eJRfM/Xr77DD85ObI/AAAAAAAADDY/KjIlWlFZExQA84VRDrMEMrB534euKAzlgCLcBGAsYHQ/s1600/NLP%2Bmodels.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2304117",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does it work? \n",
    "## Lets go over it step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4aff0e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3b3309",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e049625",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokens: ['we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library', '.']\n",
      "Token IDs: [2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012]\n",
      "Input IDs: {'input_ids': tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996,   100,\n",
      "         19081,  3075,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"We are very happy to show you the ğŸ¤— Transformers library.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "print(f\"   Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Input IDs: {encoded_input}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed9866",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now lets try multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7774c8a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996,   100,\n",
      "         19081,  3075,  1012,   102],\n",
      "        [  101,  2057,  3246,  2017,  2123,  1005,  1056,  5223,  2009,  1012,\n",
      "           102,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"We are very happy to show you the ğŸ¤— Transformers library.\",\n",
    "    \"We hope you don't hate it.\"\n",
    "]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "print (encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c563b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax predictions: tensor([[2.2043e-04, 9.9978e-01],\n",
      "        [5.3086e-01, 4.6914e-01]])\n",
      "tensor([1, 0])\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "    predictions = F.softmax(output.logits, dim=1)\n",
    "    print(f\"Softmax predictions: {predictions}\")\n",
    "    predictions = torch.argmax(output.logits, dim=1)\n",
    "    print(predictions)\n",
    "    label = [model.config.id2label[label_id] for label_id in predictions.tolist()]\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8fea5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What else can pipelines do? [Examples](https://huggingface.co/transformers/usage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b5a80",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Question Answering\n",
    "- distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78884096",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6222440600395203, 'start': 34, 'end': 95, 'answer': 'the task of extracting an answer from a text given a question'}\n",
      "{'score': 0.5115304589271545, 'start': 147, 'end': 160, 'answer': 'SQuAD dataset'}\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
    "\"\"\"\n",
    "\n",
    "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e02af5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fill mask\n",
    "- distilroberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd2fd32",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'HuggingFace is creating a tool that the community uses to solve NLP tasks.', 'score': 0.17927563190460205, 'token': 3944, 'token_str': ' tool'}, {'sequence': 'HuggingFace is creating a framework that the community uses to solve NLP tasks.', 'score': 0.11349444836378098, 'token': 7208, 'token_str': ' framework'}, {'sequence': 'HuggingFace is creating a library that the community uses to solve NLP tasks.', 'score': 0.052435252815485, 'token': 5560, 'token_str': ' library'}, {'sequence': 'HuggingFace is creating a database that the community uses to solve NLP tasks.', 'score': 0.034935373812913895, 'token': 8503, 'token_str': ' database'}, {'sequence': 'HuggingFace is creating a prototype that the community uses to solve NLP tasks.', 'score': 0.02860225923359394, 'token': 17715, 'token_str': ' prototype'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"fill-mask\")\n",
    "print(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6114352",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Name Entity Recognition\n",
    "- dbmdz/bert-large-cased-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1cfe697",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-ORG', 'score': 0.9995633, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}, {'entity': 'I-ORG', 'score': 0.9915939, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}, {'entity': 'I-ORG', 'score': 0.9982672, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}, {'entity': 'I-ORG', 'score': 0.9994404, 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}, {'entity': 'I-LOC', 'score': 0.99943465, 'index': 11, 'word': 'New', 'start': 40, 'end': 43}, {'entity': 'I-LOC', 'score': 0.99932706, 'index': 12, 'word': 'York', 'start': 44, 'end': 48}, {'entity': 'I-LOC', 'score': 0.9993865, 'index': 13, 'word': 'City', 'start': 49, 'end': 53}, {'entity': 'I-LOC', 'score': 0.9825622, 'index': 19, 'word': 'D', 'start': 79, 'end': 80}, {'entity': 'I-LOC', 'score': 0.9369829, 'index': 20, 'word': '##UM', 'start': 80, 'end': 82}, {'entity': 'I-LOC', 'score': 0.8987098, 'index': 21, 'word': '##BO', 'start': 82, 'end': 84}, {'entity': 'I-LOC', 'score': 0.97582406, 'index': 29, 'word': 'Manhattan', 'start': 113, 'end': 122}, {'entity': 'I-LOC', 'score': 0.9902494, 'index': 30, 'word': 'Bridge', 'start': 123, 'end': 129}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"ner\")\n",
    "\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
    "           \"close to the Manhattan Bridge which is visible from the window.\"\n",
    "\n",
    "print(nlp(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306783f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summarization\n",
    "- sshleifer/distilbart-cnn-12-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a004c5c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002 . At one time, she was married to eight men at once, prosecutors say .'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ecb17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to use custom pretrained models [Model Hub](https://huggingface.co/models)\n",
    "![Model Hub Image](https://media.vlpt.us/images/jaehyeong/post/a480f27e-d91a-48e5-8dc5-b670bd92c652/image.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f2b70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Generation using [KoGPT2](https://github.com/SKT-AI/KoGPT2)\n",
    "- skt/kogpt2-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9866bc7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44677c4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³µë¶€ë¥¼ ì˜ í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ìì‹ ì˜ ëŠ¥ë ¥ì„ ìµœëŒ€í•œ ë°œíœ˜í•´ì•¼ í•œë‹¤.\n",
      "ìì‹ ì˜ ëŠ¥ë ¥ìœ¼ë¡œ ì¶©ë¶„íˆ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ë°”ë¡œ ìê¸° ìì‹ ì´ë‹¤.\n",
      "ìê¸° ìì‹ ì„ ì œëŒ€ë¡œ ê³„ë°œí•˜ê¸° ìœ„í•´ì„  ë¬´ì—‡ë³´ë‹¤ ìì‹ ì´ ê°€ì§€ê³  ìˆë˜ ì¥ì ì„ ë§ˆìŒê» í¼ì¹  ì¤„ ì•Œì•„ì•¼ í•˜ë©°, ì´ë¥¼ í†µí•´ ìì‹ ì—ê²Œ ë§ëŠ” ì¼ì„ í•  ë•Œ ë¹„ë¡œì†Œ ì§„ì •í•œ ì„±ê³µì´ ë³´ì¥ëœë‹¤.\n",
      "ë˜í•œ ì´ëŸ¬í•œ ëŠ¥ë ¥ì€ ë‹¤ë¥¸ ì‚¬ëŒì˜ ë„ì›€ì„ ë°›ì§€ ì•Šê³  ìŠ¤ìŠ¤ë¡œ ë…¸ë ¥í•´ì„œ ì´ë£° ìˆ˜ë„ ìˆë‹¤.\n",
      "ë”°ë¼ì„œ ìì‹ ì— ëŒ€í•œ ê¸ì •ì ì¸ ë§ˆì¸ë“œë¥¼ ê°–ê³  ì ê·¹ì ìœ¼ë¡œ ë„ì „í•´ë³´ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\n",
      "ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ ì„±ê³µì ì¸ ì¸ìƒì„ ì‚´ìˆ˜ ìˆì„ê¹Œ?\n",
      "ë¨¼ì € ë³¸ì¸ì˜ ëŠ¥ë ¥ê³¼ ì ì¬ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ê·¸ì— ê±¸ë§ì€ ë³´ìƒì„ ë°›ëŠ” ê²ƒì´ë‹¤.\n",
      "ìì‹ ì´ ê°€ì§„ ì¥ì ì´ë‚˜ ì•½ì ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ í•œ ìì‹ ë§Œì˜ ì „ëµì„ ì„¸ì›Œì•¼ í•˜ëŠ” ê²ƒì€ ë¬¼ë¡ ì´ê³ ,\n"
     ]
    }
   ],
   "source": [
    "text = 'ê³µë¶€ë¥¼ ì˜ í•˜ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26d541",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We see that many models are doing very well\n",
    "- Carefully look at BERT\n",
    "![BERT Training](https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg)\n",
    "- We need to fine tune bert for a specific task\n",
    "    - Lets try fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98942bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Steps for fine tuning\n",
    "1. Prepare dataset\n",
    "2. Load pretrained Tokenizer\n",
    "3. Build PyTorch Dataset with encodings\n",
    "4. Load pretrained model\n",
    "5. Load trainer and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8338fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01bcb22c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/sungwookson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cf5b079e904a61ac97b5c809974fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a119039",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Load pretrained tokenizer & Build dataset with encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "979d1e77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sungwookson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-d8899e122d561bd5.arrow\n",
      "Loading cached processed dataset at /home/sungwookson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-fd5fb8cabe67605b.arrow\n",
      "Loading cached processed dataset at /home/sungwookson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-124ba69408debb19.arrow\n",
      "Loading cached shuffled indices for dataset at /home/sungwookson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-2695cd48903e6005.arrow\n",
      "Loading cached shuffled indices for dataset at /home/sungwookson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-56eb74918e2724ef.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_fn(x):\n",
    "    return tokenizer(x['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_fn, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)) \n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000)) \n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efa56591",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# In tranditional pytorch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc406638",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42f46a75",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73990bf5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Load trainer and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c627bd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.2855883382161458, metrics={'train_runtime': 35.3371, 'train_samples_per_second': 84.897, 'train_steps_per_second': 10.612, 'total_flos': 397402195968000.0, 'train_loss': 0.2855883382161458, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\")\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca8d3ce2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb9bae061944717b94199c266a91cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In traditional pytorch\n",
    "\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1478e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to save and load model / tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64f5b8ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ckpt/config.json\n",
      "Model weights saved in ckpt/pytorch_model.bin\n",
      "tokenizer config file saved in ckpt/tokenizer_config.json\n",
      "Special tokens file saved in ckpt/special_tokens_map.json\n",
      "loading configuration file ckpt/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ckpt/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "Didn't find file ckpt/added_tokens.json. We won't load it.\n",
      "loading file ckpt/vocab.txt\n",
      "loading file ckpt/tokenizer.json\n",
      "loading file None\n",
      "loading file ckpt/special_tokens_map.json\n",
      "loading file ckpt/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "save_directory = os.path.join('ckpt')\n",
    "\n",
    "#Saving\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "#Loading\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee77be4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
