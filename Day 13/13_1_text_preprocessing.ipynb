{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_1_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyME/A7PvIOXIgywweo34tWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SLCFLAB/Data-Science-Python/blob/main/Day%2013/13_1_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "In this notebook, you will practice how to preprocess text data before analyzing it. It's an inevitable process for every NLP project.\n",
        "\n",
        "\n",
        "Reference:    \n",
        "* https://github.com/gilbutITbook/080289/blob/main/chap09/colab_9%EC%9E%A5.ipynb"
      ],
      "metadata": {
        "id": "F4MTwthBi9u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS8XxK7xeCcq",
        "outputId": "8c680587-d7b0-4a46-c64a-0f41fc644818"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J41RYnLEdo9Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"popular\")\n",
        "text=nltk.word_tokenize(\"Is it possible distinguishing cats and dogs\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgdqEh0qeKok",
        "outputId": "2199ccb3-03e2-4508-9f8d-4321ecc6c680"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Is', 'it', 'possible', 'distinguishing', 'cats', 'and', 'dogs']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xknjbw0FeMzT",
        "outputId": "4ebd8399-3cd6-4d82-92f7-95e126ce40d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8DUfpOnepug",
        "outputId": "efd25ab9-19e6-4f45-d1c0-3e02a4dd684b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Is', 'VBZ'),\n",
              " ('it', 'PRP'),\n",
              " ('possible', 'JJ'),\n",
              " ('distinguishing', 'VBG'),\n",
              " ('cats', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('dogs', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "string1=\"my favorite subject is math\"\n",
        "string2=\"my favorite subject is math, english, economic and computer science\"\n",
        "nltk.word_tokenize(string1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsjt5OFnerQO",
        "outputId": "65145bc2-ce53-4348-b523-19cb1e030b41"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'favorite', 'subject', 'is', 'math']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(string2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKx2I1Sjewqa",
        "outputId": "5040a294-756c-47b6-dfaf-fa1e3006aea8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my',\n",
              " 'favorite',\n",
              " 'subject',\n",
              " 'is',\n",
              " 'math',\n",
              " ',',\n",
              " 'english',\n",
              " ',',\n",
              " 'economic',\n",
              " 'and',\n",
              " 'computer',\n",
              " 'science']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KoNLPy\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "print(komoran.morphs('딥러닝이 쉽나요? 어렵나요?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJyVCOr-ey8g",
        "outputId": "6985d35a-7ef7-445c-a792-70a5454f0dd3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥러닝이', '쉽', '나요', '?', '어렵', '나요', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(komoran.pos('소파 위에 있는 것이 고양이인가요? 강아지인가요?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-dPiCPme5qm",
        "outputId": "a4a445d8-937f-4e24-fb29-265e4ef68e3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('소파', 'NNP'), ('위', 'NNG'), ('에', 'JKB'), ('있', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('이', 'JKS'), ('고양이', 'NNG'), ('이', 'VCP'), ('ㄴ가요', 'EF'), ('?', 'SF'), ('강아지', 'NNG'), ('이', 'VCP'), ('ㄴ가요', 'EF'), ('?', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "RWitviLqe-r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenizing\n",
        "from nltk import sent_tokenize\n",
        "text_sample = 'Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language. In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.'\n",
        "tokenized_sentences = sent_tokenize(text_sample)\n",
        "print(tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdS3HUnxfDCI",
        "outputId": "33e238a7-2ec3-429d-a807-0f5ecdc3a55f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural Language Processing, or NLP, is the process of extracting the meaning, or intent, behind human language.', 'In the field of Conversational artificial intelligence (AI), NLP allows machines and applications to understand the intent of human language inputs, and then generate appropriate responses, resulting in a natural conversation flow.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenizing\n",
        "from nltk import word_tokenize\n",
        "sentence = \" This book is for deep learning learners\"\n",
        "words = word_tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3em995E-fFFm",
        "outputId": "40462402-bc32-4a6b-850d-db84e04fc7cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'book', 'is', 'for', 'deep', 'learning', 'learners']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing w.r.t. apostrophe\n",
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "sentence = \"it’s nothing that you don’t already know except most people aren’t aware of how their inner world works.\"\n",
        "words = WordPunctTokenizer().tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzOR6OnqfIWq",
        "outputId": "8eab0015-c3f6-47a6-bf52-edfe91528f72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['it', '’', 's', 'nothing', 'that', 'you', 'don', '’', 't', 'already', 'know', 'except', 'most', 'people', 'aren', '’', 't', 'aware', 'of', 'how', 'their', 'inner', 'world', 'works', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://raw.githubusercontent.com/gilbutITbook/080289/main/chap09/data/ratings_train.txt\""
      ],
      "metadata": {
        "id": "aevCtEmDfZgv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_data = pd.read_csv(data_url, sep='\\t')"
      ],
      "metadata": {
        "id": "LAsJVRG5fpRS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HkwR9-5Tfyh6",
        "outputId": "17f0b95c-1a3d-4560-aef5-ff79cca2e066"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2ea145f-403c-4884-b620-e79cb2bed1ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2ea145f-403c-4884-b620-e79cb2bed1ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2ea145f-403c-4884-b620-e79cb2bed1ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2ea145f-403c-4884-b620-e79cb2bed1ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(rating_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-wpnoDxgWV5",
        "outputId": "cbeb2ae3-d48b-48f2-f049-fb45c1b8e16f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "yAt91xv-f849"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "lRTwKDcyhKov"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter(Okt) - 오픈 소스 한글 형태소 분석기\n",
        "okt = Okt()\n",
        "\n",
        "result = []\n",
        "for comment in tqdm(rating_data.document.values[:1000], desc='형태소 분석 중...'):\n",
        "  malist = okt.pos(comment, norm=True, stem=True)\n",
        "  r = []\n",
        "  for word in malist:\n",
        "    if not word[1] in 'Josa, Eomi, Punctuation'.split(', '):\n",
        "      r.append(word[0])\n",
        "  rl = (' '.join(r)).strip()\n",
        "  result.append(rl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5Z7OczjgGO1",
        "outputId": "1e116ecf-0793-4e11-8dc7-cce6d475adfa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "형태소 분석 중...: 100%|██████████| 1000/1000 [00:06<00:00, 145.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w88lyjsyh82O",
        "outputId": "346e5b6c-91c6-4e03-8590-cdfd0a2906ea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아 더빙 진짜 짜증나다 목소리',\n",
              " '흠 포스터 보고 초딩 영화 줄 오버 연기 가볍다 않다',\n",
              " '너 무재 밓었 다그 래서 보다 추천 다',\n",
              " '교도소 이야기 구먼 솔직하다 재미 없다 평점 조정',\n",
              " '사이 몬페 그 의 익살스럽다 연기 돋보이다 영화 스파이더맨 늙다 보이다 하다 커스틴 던스트 너무나도 이쁘다 보이다',\n",
              " '막 걸음 마 떼다 3 세 초등학교 1 학년 생인 8 살다 영화 ㅋㅋㅋ 별 반개 아깝다 움',\n",
              " '원작 긴장감 제대로 살리다 하다',\n",
              " '별 반개 아깝다 욕 나오다 이응경 길용우 연 기 생활 몇 년 정말 발 해도 그것 낫다 납치 감금 반복 반복 이 드라마 가족 없다 연기 못 하다 사람 모 엿 네',\n",
              " '액션 없다 재미 있다 몇 안되다 영화',\n",
              " '왜 이렇게 평점 낮다 꽤 볼 한 데 헐리우드 식 화려하다 너무 길들이다 있다']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Stopwords"
      ],
      "metadata": {
        "id": "bzCQI5vUhDe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sample_text = \"One of the first things that we ask ourselves is what are the pros and cons of any task we perform.\"\n",
        "text_tokens = word_tokenize(sample_text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "print(\"불용어 제거 미적용:\", text_tokens, '\\n')\n",
        "print(\"불용어 제거 적용:\",tokens_without_sw)"
      ],
      "metadata": {
        "id": "flt5FCp8hg89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "FfxMSDDthslW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex) writing, writes, wrote, written -> write"
      ],
      "metadata": {
        "id": "1rdzqkBghxvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter algorithm\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem('obesses'),stemmer.stem('obssesed'))\n",
        "print(stemmer.stem('standardizes'),stemmer.stem('standardization'))\n",
        "print(stemmer.stem('national'), stemmer.stem('nation'))\n",
        "print(stemmer.stem('absentness'), stemmer.stem('absently'))\n",
        "print(stemmer.stem('tribalical'), stemmer.stem('tribalicalized'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQQN8TIFh2TG",
        "outputId": "5f5c10cc-1fc9-4c83-aa61-fc2163d2447f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obess obsses\n",
            "standard standard\n",
            "nation nation\n",
            "absent absent\n",
            "tribal tribalic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('obsesses'),stemmer.stem('obsessed'))\n",
        "print(stemmer.stem('standardizes'),stemmer.stem('standardization'))\n",
        "print(stemmer.stem('national'), stemmer.stem('nation'))\n",
        "print(stemmer.stem('absentness'), stemmer.stem('absently'))\n",
        "print(stemmer.stem('tribalical'), stemmer.stem('tribalicalized'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T27URSuiNoO",
        "outputId": "dbb7d94a-a979-4065-9c6e-8b32d9464e72"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obsess obsess\n",
            "standard standard\n",
            "nat nat\n",
            "abs abs\n",
            "trib trib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(stemmer.stem('obsesses'),stemmer.stem('obsessed'))\n",
        "print(lemma.lemmatize('standardizes'),lemma.lemmatize('standardization'))\n",
        "print(lemma.lemmatize('national'), lemma.lemmatize('nation'))\n",
        "print(lemma.lemmatize('absentness'), lemma.lemmatize('absently'))\n",
        "print(lemma.lemmatize('tribalical'), lemma.lemmatize('tribalicalized'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbqAjpaviP4t",
        "outputId": "8e1657ae-796c-40bd-ddfd-e79bb6f893c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "obsess obsess\n",
            "standardizes standardization\n",
            "national nation\n",
            "absentness absently\n",
            "tribalical tribalicalized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemma.lemmatize('obsesses', 'v'),lemma.lemmatize('obsessed','a'))\n",
        "print(lemma.lemmatize('standardizes','v'),lemma.lemmatize('standardization','n'))\n",
        "print(lemma.lemmatize('national','a'), lemma.lemmatize('nation','n'))\n",
        "print(lemma.lemmatize('absentness','n'), lemma.lemmatize('absently','r'))\n",
        "print(lemma.lemmatize('tribalical','a'), lemma.lemmatize('tribalicalized','v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tLJ7lCyiVDG",
        "outputId": "f431f432-0525-4e5d-960e-7eea5cb436ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obsess obsessed\n",
            "standardize standardization\n",
            "national nation\n",
            "absentness absently\n",
            "tribalical tribalicalized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To-do"
      ],
      "metadata": {
        "id": "3yuQWfRaiZEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using requests and bs4 library, get today's lunch and dinner menu of '두레미담' then pre-process them."
      ],
      "metadata": {
        "id": "NztdiB0Bj3wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "Au-sClNYlI9l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://snuco.snu.ac.kr/foodmenu\""
      ],
      "metadata": {
        "id": "c_yEzOgFlPRj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = requests.get(url)"
      ],
      "metadata": {
        "id": "CiG1eLvrlyIx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html.text, 'html.parser')"
      ],
      "metadata": {
        "id": "Z24ezfH_l2JS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lunch = soup.find_all('td', {'class': 'views-field views-field-field-lunch'})"
      ],
      "metadata": {
        "id": "3-oxA5mdmCBk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두레미담: 5th restaurant\n",
        "lunch[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5O4y9ximfVS",
        "outputId": "dd6ea69c-6b8c-4b97-d055-aa124eae5eed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<td class=\"views-field views-field-field-lunch\">\n",
              "<p>&lt;셀프코너&gt; 6,500원</p>\n",
              "<p>잡곡밥</p>\n",
              "<p>고기짬뽕탕</p>\n",
              "<p>동파육</p>\n",
              "<p>찹쌀꿔바로우&amp;조청소스</p>\n",
              "<p>사천식피쉬볼볶음</p>\n",
              "<p> </p>\n",
              "<p>유부맛살냉채</p>\n",
              "<p>깍두기</p>\n",
              "<p>오늘의차/그린샐러드</p>\n",
              "<p> </p>\n",
              "<p>&lt;주문식 메뉴&gt;</p>\n",
              "<p>고등어 소금구이 13,000원</p>\n",
              "<p>철판주꾸미 볶음 14,000원</p>\n",
              "<p>떡갈비 구이 15,000원</p>\n",
              "<p>도가니탕 18,000원</p>\n",
              "<p>돌솥 7선 산채비빔밥 15,000원</p>\n",
              "<p>차돌된장찌개 10,000원</p>\n",
              "<p>초계국수/냉면+왕만두 11,000원</p>\n",
              "<p>솥밥(추가) 2,000원</p>\n",
              "<p> </p>\n",
              "<p><span style=\"font-size: 14px;\"><span style='color: black; font-family: \"맑은 고딕\"; language: en-US; mso-ascii-font-family: \"맑은 고딕\"; mso-fareast-font-family: \"맑은 고딕\"; mso-bidi-font-family: +mj-cs; mso-ascii-theme-font: major-latin; mso-fareast-theme-font: major-fareast; mso-bidi-theme-font: major-bidi; mso-color-index: 1; mso-font-kerning: 12.0pt; mso-style-textfill-type: solid; mso-style-textfill-fill-themecolor: text1; mso-style-textfill-fill-color: black; mso-style-textfill-fill-alpha: 100.0%;'>※</span><span style='color: black; font-family: \"맑은 고딕\"; language: ko; mso-ascii-font-family: \"맑은 고딕\"; mso-fareast-font-family: \"맑은 고딕\"; mso-bidi-font-family: +mj-cs; mso-ascii-theme-font: major-latin; mso-fareast-theme-font: major-fareast; mso-bidi-theme-font: major-bidi; mso-color-index: 1; mso-font-kerning: 12.0pt; mso-style-textfill-type: solid; mso-style-textfill-fill-themecolor: text1; mso-style-textfill-fill-color: black; mso-style-textfill-fill-alpha: 100.0%;'>운영시간</span><span style='color: black; font-family: \"맑은 고딕\"; language: en-US; mso-ascii-font-family: \"맑은 고딕\"; mso-fareast-font-family: \"맑은 고딕\"; mso-bidi-font-family: +mj-cs; mso-ascii-theme-font: major-latin; mso-fareast-theme-font: major-fareast; mso-bidi-theme-font: major-bidi; mso-color-index: 1; mso-font-kerning: 12.0pt; mso-style-textfill-type: solid; mso-style-textfill-fill-themecolor: text1; mso-style-textfill-fill-color: black; mso-style-textfill-fill-alpha: 100.0%;'>: 11:00~14:00<br/>\n",
              "※</span><span style='color: black; font-family: \"맑은 고딕\"; language: ko; mso-ascii-font-family: \"맑은 고딕\"; mso-fareast-font-family: \"맑은 고딕\"; mso-bidi-font-family: +mj-cs; mso-ascii-theme-font: major-latin; mso-fareast-theme-font: major-fareast; mso-bidi-theme-font: major-bidi; mso-color-index: 1; mso-font-kerning: 12.0pt; mso-style-textfill-type: solid; mso-style-textfill-fill-themecolor: text1; mso-style-textfill-fill-color: black; mso-style-textfill-fill-alpha: 100.0%;'>혼잡시간</span><span style='color: black; font-family: \"맑은 고딕\"; language: en-US; mso-ascii-font-family: \"맑은 고딕\"; mso-fareast-font-family: \"맑은 고딕\"; mso-bidi-font-family: +mj-cs; mso-ascii-theme-font: major-latin; mso-fareast-theme-font: major-fareast; mso-bidi-theme-font: major-bidi; mso-color-index: 1; mso-font-kerning: 12.0pt; mso-style-textfill-type: solid; mso-style-textfill-fill-themecolor: text1; mso-style-textfill-fill-color: black; mso-style-textfill-fill-alpha: 100.0%;'>: 12:00~12:30</span></span></p>\n",
              "</td>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_tag_list = lunch[4].find_all('p')\n",
        "\n",
        "menus = []\n",
        "for idx, item in enumerate(p_tag_list[1:]):\n",
        "  menu = item.get_text()\n",
        "  if menu != '\\xa0':\n",
        "    menus.append(menu)\n",
        "  if menu == '오늘의차/그린샐러드':\n",
        "    break"
      ],
      "metadata": {
        "id": "iL5UMINCm1OX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "menus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcic_0EOohko",
        "outputId": "4d2a1a02-26d2-4cb5-8568-08c294c82766"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['잡곡밥',\n",
              " '고기짬뽕탕',\n",
              " '동파육',\n",
              " '찹쌀꿔바로우&조청소스',\n",
              " '사천식피쉬볼볶음',\n",
              " '유부맛살냉채',\n",
              " '깍두기',\n",
              " '오늘의차/그린샐러드']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now complete rest of the task"
      ],
      "metadata": {
        "id": "spS7VWlZpcwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oQ8Nmhl-pqir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RVlhACVHprCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "quwsCYTVprSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}